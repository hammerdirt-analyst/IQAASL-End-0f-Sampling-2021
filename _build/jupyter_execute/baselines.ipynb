{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "# This is a report using the data from IQAASL.\n",
    "# IQAASL was a project funded by the Swiss Confederation\n",
    "# It produces a summary of litter survey results for a defined region.\n",
    "# These charts serve as the models for the development of plagespropres.ch\n",
    "# The data is gathered by volunteers.\n",
    "# Please remember all copyrights apply, please give credit when applicable\n",
    "# The repo is maintained by the community effective January 01, 2022\n",
    "# There is ample opportunity to contribute, learn and teach\n",
    "# contact dev@hammerdirt.ch\n",
    "\n",
    "# Dies ist ein Bericht, der die Daten von IQAASL verwendet.\n",
    "# IQAASL war ein von der Schweizerischen Eidgenossenschaft finanziertes Projekt.\n",
    "# Es erstellt eine Zusammenfassung der Ergebnisse der Littering-Umfrage für eine bestimmte Region.\n",
    "# Diese Grafiken dienten als Vorlage für die Entwicklung von plagespropres.ch.\n",
    "# Die Daten werden von Freiwilligen gesammelt.\n",
    "# Bitte denken Sie daran, dass alle Copyrights gelten, bitte geben Sie den Namen an, wenn zutreffend.\n",
    "# Das Repo wird ab dem 01. Januar 2022 von der Community gepflegt.\n",
    "# Es gibt reichlich Gelegenheit, etwas beizutragen, zu lernen und zu lehren.\n",
    "# Kontakt dev@hammerdirt.ch\n",
    "\n",
    "# Il s'agit d'un rapport utilisant les données de IQAASL.\n",
    "# IQAASL était un projet financé par la Confédération suisse.\n",
    "# Il produit un résumé des résultats de l'enquête sur les déchets sauvages pour une région définie.\n",
    "# Ces tableaux ont servi de modèles pour le développement de plagespropres.ch\n",
    "# Les données sont recueillies par des bénévoles.\n",
    "# N'oubliez pas que tous les droits d'auteur s'appliquent, veuillez indiquer le crédit lorsque cela est possible.\n",
    "# Le dépôt est maintenu par la communauté à partir du 1er janvier 2022.\n",
    "# Il y a de nombreuses possibilités de contribuer, d'apprendre et d'enseigner.\n",
    "# contact dev@hammerdirt.ch\n",
    "\n",
    "# sys, file and nav packages:\n",
    "import datetime as dt\n",
    "import locale\n",
    "# math packages:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from statsmodels.stats.stattools import medcouple\n",
    "from scipy.optimize import newton\n",
    "from scipy.special import digamma\n",
    "import math\n",
    "\n",
    "# charting:\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import ticker\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "# home brew utitilties\n",
    "import resources.chart_kwargs as ck\n",
    "import resources.sr_ut as sut\n",
    "\n",
    "# build report\n",
    "import reportlab\n",
    "from reportlab.platypus.flowables import Flowable\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph,PageBreak, KeepTogether\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.lib.units import cm\n",
    "from reportlab.platypus import Table, TableStyle\n",
    "\n",
    "# the module that has all the methods for handling the data\n",
    "import resources.featuredata as featuredata\n",
    "from resources.featuredata import makeAList, small_space, large_space, aSingleStyledTable, smallest_space\n",
    "from resources.featuredata import caption_style, subsection_title, title_style, block_quote_style\n",
    "from resources.featuredata import figureAndCaptionTable, tableAndCaption, aStyledTableWithTitleRow\n",
    "from resources.featuredata import sectionParagraphs, section_title, addToDoc, makeAParagraph, bold_block\n",
    "\n",
    "# images and display\n",
    "import IPython\n",
    "from IPython.display import HTML\n",
    "import matplotlib.image as mpimg\n",
    "from myst_nb import glue\n",
    "\n",
    "loc = locale.getlocale()\n",
    "lang =  \"de_CH.utf8\"\n",
    "locale.setlocale(locale.LC_ALL, lang)\n",
    "\n",
    "# set some parameters:\n",
    "start_date = '2020-03-01'\n",
    "end_date ='2021-05-31'\n",
    "\n",
    "# name of the output folder:\n",
    "name_of_project = 'threshold_values'\n",
    "save_fig_prefix = \"resources/output/\"\n",
    "\n",
    "save_figure_kwargs = {\n",
    "    \"fname\": None,\n",
    "    \"dpi\": 300.0,\n",
    "    \"format\": \"jpeg\",\n",
    "    \"bbox_inches\": None,\n",
    "    \"pad_inches\": 0,\n",
    "    \"bbox_inches\": 'tight',\n",
    "    \"facecolor\": 'auto',\n",
    "    \"edgecolor\": 'auto',\n",
    "    \"backend\": None,\n",
    "}\n",
    "\n",
    "\n",
    "# the scale for pieces per meter and the column and chart label for the units\n",
    "reporting_unit = 100\n",
    "unit_label = 'p/100 m'\n",
    "\n",
    "# get your data:\n",
    "survey_data = pd.read_csv('resources/checked_sdata_eos_2020_21.csv')\n",
    "dfBeaches = pd.read_csv(\"resources/beaches_with_land_use_rates.csv\")\n",
    "dfCodes = pd.read_csv(\"resources/codes_with_group_names_2015.csv\")\n",
    "dfDims = pd.read_csv(\"resources/corrected_dims.csv\")\n",
    "\n",
    "# set the index of the beach data to location slug\n",
    "dfBeaches.set_index('location', inplace=True)\n",
    "\n",
    "# index the code data\n",
    "dfCodes.set_index(\"code\", inplace=True)\n",
    "\n",
    "# make a map to the code descriptions\n",
    "code_description_map = dfCodes.description\n",
    "\n",
    "# make a map to the code descriptions\n",
    "code_material_map = dfCodes.material\n",
    "\n",
    "# this defines the css rules for the note-book table displays\n",
    "header_row = {'selector': 'th:nth-child(1)', 'props': f'background-color: #FFF; font-size:20px; text-align:left;'}\n",
    "even_rows = {\"selector\": 'tr:nth-child(even)', 'props': f'background-color: rgba(139, 69, 19, 0.08);'}\n",
    "odd_rows = {'selector': 'tr:nth-child(odd)', 'props': 'background: #FFF;'}\n",
    "table_font = {'selector': 'tr', 'props': 'font-size: 20px;'}\n",
    "table_data = {'selector': 'td', 'props': 'padding:6px;'}\n",
    "table_css_styles = [even_rows, odd_rows, table_font, header_row, table_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "(threshhold)=\n",
    "# Basiswerte für Abfallobjekte an Gewässern\n",
    "\n",
    "{Download}`Download </resources/pdfs/baselines.pdf>`\n",
    "\n",
    "\n",
    "Basiswerte (BVs), die auch als Benchmarks bezeichnet werden, sind die Mengen oder Werte, die zur statistischen Definition einer Situation verwendet werden. Die BVs beziehen sich auf eine Reihe von Daten, die sowohl zeitlich als auch geografisch begrenzt sind und auch als Referenzpunkt oder Basisperiode bezeichnet werden. BVs sind die Grössen, an denen der Fortschritt gemessen wird. In dieser Hinsicht sind die BVs eng mit den Daten und den zu ihrer Erhebung verwendeten Methoden verbunden.\n",
    "\n",
    "## Zählen von Abfallobjekten am Strand: ein Überblick\n",
    "\n",
    "Der erste internationale Leitfaden zur Erfassung von Abfallobjekten am Strand wurde 2008 vom Umweltprogramm der Vereinten Nationen (UNEP) und der Zwischenstaatlichen Ozeanographischen Kommission (IOC) veröffentlicht {cite}`unepseas`. Auf der Grundlage der gesammelten Arbeit vieler Forschenden wurde diese Methode 2010 von der OSPAR-Kommission übernommen {cite}`ospard10`,  2013 veröffentlichte die EU einen Leitfaden für die Überwachung mariner Abfallobjekte an den europäischen Meeren (_the guide_) {cite}`mlwguidance`. Die Schweiz ist Mitglied von OSPAR und hat über 1000 Proben nach den im the guide beschriebenen Methoden genommen.\n",
    "\n",
    "Auf _the guide_ folgte 2016 Riverine Litter Monitoring - Options and Recommendations {cite}`riverinemonitor`, der die zunehmende Erkenntnis widerspiegelt, dass Flüsse wichtige Quellen für Haushaltsabfälle in Küstenregionen sind. Zu diesem Zeitpunkt war das erste Projekt zur Überwachung von Abfallobjekten am Genfer See bereits abgeschlossen und die Vorbereitungen für ein einjähriges nationales Projekt in der Schweiz, das von STOPPP initiiert und von WWF-Freiwilligen unterstützt wurde, liefen an, siehe [Vergleich der Datenerhebungen seit 2018](slr-iqaasl).\n",
    "\n",
    "2019 veröffentlichte das Joint Research Centre (JRC) eine Analyse eines europäischen Datensatzes zu Abfallobjekten am Strand aus den Jahren 2012–2016, ein technisches Dokument, in dem die Methoden und verschiedenen Szenarien zur Berechnung von Basiswerten von Abfallobjekten am Strand detailliert beschrieben werden. Von besonderem Interesse für das JRC war die Robustheit der Methoden gegenüber Extremwerten und die Transparenz der Berechnungsmethode.{cite}`eubaselines`\n",
    "\n",
    "Im September 2020 schliesslich legte die EU Basis- und Zielwerte auf der Grundlage der 2015–2016 erhobenen Daten fest. Die Zielwerte beziehen sich auf den guten Umweltzustand der Meeresgewässer, der in der Meeresstrategie-Rahmenrichtlinie (MSRL) beschrieben wird. Die Basiswerte wurden anhand der in der Veröffentlichung von 2019 beschriebenen Methoden berechnet.{cite}`threshholdeu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "pdfcomponents = []\n",
    "\n",
    "the_title = Paragraph(\"Basiswerte für Abfallobjekte an Gewässern\", title_style)\n",
    "\n",
    "p_one = [\n",
    "    \"Basiswerte (BVs), die auch als Benchmarks bezeichnet werden, sind die Mengen oder Werte, die zur statistischen \",\n",
    "    \"Definition einer Situation verwendet werden. Die BVs beziehen sich auf eine Reihe von Daten, die sowohl zeitlich \",\n",
    "    \"als auch geografisch begrenzt sind und auch als Referenzpunkt oder Basisperiode bezeichnet werden. BVs sind die \",\n",
    "    \"Grössen, an denen der Fortschritt gemessen wird. In dieser Hinsicht sind die BVs eng mit den Daten und den zu \",\n",
    "    \"ihrer Erhebung verwendeten Methoden verbunden.\"\n",
    "]\n",
    "\n",
    "section_one_title = Paragraph(\"Zählen von Abfallobjekten am Strand: ein Überblick\", section_title)\n",
    "\n",
    "p_two = [\n",
    "    \"Der erste internationale Leitfaden zur Erfassung von Abfallobjekten am Strand wurde 2008 vom Umweltprogramm der \",\n",
    "    \"Vereinten Nationen (UNEP) und der Zwischenstaatlichen Ozeanographischen Kommission (IOC) veröffentlicht \",\n",
    "    '<a href=\"#eall09\" color=\"blue\">(eall09)</a>.',\n",
    "    \"Auf der Grundlage der gesammelten Arbeit vieler Forschenden wurde diese Methode 2010 von der OSPAR-Kommission \",\n",
    "    'übernommen <a href=\"#OSP17\" color=\"blue\">(OSP17)</a>. 2013 veröffentlichte die EU einen Leitfaden für die Überwachung mariner Abfallobjekte an den ',\n",
    "    'europäischen Meeren (the guide) <a href=\"#Han13\" color=\"blue\"> (Han13) </a>. Die Schweiz ist Mitglied von OSPAR und hat über 1000 Proben nach den ',\n",
    "    \"im the guide beschriebenen Methoden genommen.\"\n",
    "]\n",
    "\n",
    "p_one = makeAParagraph(p_one)\n",
    "\n",
    "p_three = [\n",
    "    'Auf the guide folgte 2016 Riverine Litter Monitoring - Options and Recommendations <a href=\"#HGFT17\" color=\"blue\">(HGFT)</a>, der die zunehmende ',\n",
    "    \"Erkenntnis widerspiegelt, dass Flüsse wichtige Quellen für Haushaltsabfälle in Küstenregionen sind. Zu diesem \",\n",
    "    \"Zeitpunkt war das erste Projekt zur Überwachung von Abfallobjekten am Genfer See bereits abgeschlossen und die \",\n",
    "    \"Vorbereitungen für ein einjähriges nationales Projekt in der Schweiz, das von STOPPP initiiert und von WWF-Freiwilligen \",\n",
    "    'unterstützt wurde, liefen an, siehe <a href=\"https://hammerdirt-analyst.github.io/IQAASL-End-0f-Sampling-2021/slr_2017.html\" color=\"blue\"> Vergleich der Datenerhebungen seit 2018 </a>.'\n",
    "]\n",
    "\n",
    "p_four = [\n",
    "    \"2019 veröffentlichte das Joint Research Centre (JRC) eine Analyse eines europäischen Datensatzes zu Abfallobjekten \",\n",
    "    \"am Strand aus den Jahren 2012–2016, ein technisches Dokument, in dem die Methoden und verschiedenen Szenarien zur \",\n",
    "    \"Berechnung von Basiswerten von Abfallobjekten am Strand detailliert beschrieben werden. Von besonderem Interesse für \",\n",
    "    \"das JRC war die Robustheit der Methoden gegenüber Extremwerten und die Transparenz der Berechnungsmethode.\",\n",
    "    '<a href=\"#HG19\" color=\"blue\">(HG19)</a>'\n",
    "]\n",
    "\n",
    "p_five = [\n",
    "    \"Im September 2020 schliesslich legte die EU Basis- und Zielwerte auf der Grundlage der 2015–2016 erhobenen Daten fest. \",\n",
    "    \"Die Zielwerte beziehen sich auf den guten Umweltzustand der Meeresgewässer, der in der Meeresstrategie-Rahmenrichtlinie (MSRL) \",\n",
    "    \"beschrieben wird. Die Basiswerte wurden anhand der in der Veröffentlichung von 2019 beschriebenen Methoden berechnet.\",\n",
    "    '<a href=\"#VLW20\" color=\"blue\">(VLW20)</a>'\n",
    "]\n",
    "\n",
    "acap = Paragraph(\"Zählen von Abfallobjekten am 18.05.2020 am Zürichsee, Richterswil; 3,49 Objekte pro Meter.\", caption_style),\n",
    "\n",
    "figure_kwargs = {\n",
    "    \"image_file\":\"resources/images/baselines/takingnotes2.jpg\",\n",
    "    \"caption\": acap, \n",
    "    \"original_width\":142.24,\n",
    "    \"original_height\":106.68,\n",
    "    \"desired_width\": 8.5,\n",
    "    \"caption_height\":1,\n",
    "    \"hAlign\": \"LEFT\",\n",
    "}\n",
    "\n",
    "first_figure = figureAndCaptionTable(**figure_kwargs)\n",
    "\n",
    "p_two_to_five = sectionParagraphs([p_two, p_three, p_four, p_five], smallspace=smallest_space)\n",
    "\n",
    "subsec_2_title = Paragraph(\"Schweiz 2020\", subsection_title)\n",
    "\n",
    "p_six = [\n",
    "    \"Das IQAASL-Projekt begann im März 2020. Erhebungsorte in den definierten Erhebungsgebieten wurden 2017 \",\n",
    "    \"beprobt oder neu eingerichtet. Ähnlich wie die Erhebungsergebnisse in der Meeresumwelt sind die Daten über \",\n",
    "    \"Ufer-Abfallobjekte in der Schweiz sehr unterschiedlich. Die Werte reichen von Null bis zu Tausenden von \",\n",
    "    \"Objekten und Fragmenten innerhalb von 100 m Distanz entlang von Fluss- oder Seeufer.\"    \n",
    "]\n",
    "\n",
    "p_seven = [\n",
    "    \"The guide ist die Referenz für dieses Projekt. Die Geographie der Schweiz besteht jedoch nicht aus langen, \",\n",
    "    \"homogenen Küstenabschnitten. Daher müssen bestimmte Empfehlungen in den Kontext der lokalen Topographie gestellt werden: \",\n",
    "    \"<b>Die empfohlene Länge bleibt bei 100 m, je nach Region ist dies jedoch nicht immer möglich.</b>\"\n",
    "]\n",
    "\n",
    "schweiz_2020 = makeAParagraph(p_six)\n",
    "the_guide = makeAParagraph(p_seven)\n",
    "figure_text = [\n",
    "    [first_figure, [schweiz_2020, smallest_space, the_guide]],\n",
    "   \n",
    "]\n",
    "\n",
    "side_by_side = Table(figure_text, style=featuredata.side_by_side_style_figure_left, colWidths=[8.5*cm, 8*cm])\n",
    "\n",
    "# references\n",
    "refs = Paragraph(\"References\", title_style)\n",
    "eall09 = [\n",
    "    '<a name=\"eall09\"/>eall09: <i>Cheshire et all, </i> Unep/ioc guidelines on survey and monitoring of marine litter. UNEP Regional Seas Reports and Studies, 2009.'\n",
    "]\n",
    "OSP17 = [\n",
    "    '<a name=\"OSP17\"/>OSP17: <i>OSPAR</i>, Beach litter - abundance, composition and trends. D10 - Marine Litter, 2017.'\n",
    "]\n",
    "\n",
    "Han13 = [\n",
    "    '<a name=\"Han13\"/>Han13: <i>George Hanke</i>. Guidance on monitoring of marine litter in european seas. Joint Research Centre of the European Commission, 2013.'\n",
    "]\n",
    "HGFT17 = [\n",
    "    '<a name=\"HGFT17\"/>HGFT17: <i>Georg Hanke, Daniel González Fernández, Tweehuysen, Bellert, Holzhauer, Andreja Palatinus, Hohenblum, and L. Oosterbaan.</i> ',\n",
    "    \"Riverine litter monitoring - options and recommendations. publications Office of the European Union, 02 2017. doi:10.2788/461233.\"\n",
    "]\n",
    "\n",
    "HG19 = [\n",
    "    '<a name=\"HG19\" />HG19: <i> Van Loon W. Hanke G., Walvoort D.</i> Eu marine beach litter baselines. Publications Office of the European Union, 2019. doi:10.2760/16903.'\n",
    "]\n",
    "\n",
    "vlw20_team = \"Fleet D. Van Loon W., Hanke G.\"\n",
    "vlw20pub = [\n",
    "    \"A european threshold value and assessment method for macro litter on coastlines. Publications Office of the European Union, 2020.\"\n",
    "]\n",
    "vlw20pub = ''.join(vlw20pub)\n",
    "vlw20 = makeAParagraph(featuredata.makeBibEntry(name=\"VLW20\", team=vlw20_team, pub=vlw20pub))\n",
    "\n",
    "cheshire = makeAParagraph(eall09)\n",
    "ospar17 = makeAParagraph(OSP17)\n",
    "han13 = makeAParagraph(Han13)\n",
    "hgft17 = makeAParagraph(HGFT17)\n",
    "hg19 = makeAParagraph(HG19)\n",
    "\n",
    "\n",
    "references = [PageBreak(),\n",
    "    refs,\n",
    "    small_space,\n",
    "    cheshire,\n",
    "    smallest_space,\n",
    "    ospar17,\n",
    "    smallest_space,\n",
    "    han13,\n",
    "    smallest_space,\n",
    "    hgft17,\n",
    "    smallest_space,\n",
    "    hg19,\n",
    "    smallest_space,\n",
    "    vlw20\n",
    "]\n",
    "\n",
    "new_components = [\n",
    "    the_title,\n",
    "    small_space,\n",
    "    p_one,\n",
    "    smallest_space,\n",
    "    section_one_title,\n",
    "    small_space,\n",
    "    *p_two_to_five,\n",
    "    smallest_space,\n",
    "    subsec_2_title,\n",
    "    small_space,\n",
    "    side_by_side,\n",
    "    \n",
    "]\n",
    "\n",
    "pdfcomponents = addToDoc(new_components, pdfcomponents)\n",
    "\n",
    "glue(\"blank_caption\", \" \", display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{figure} resources/images/baselines/takingnotes2.jpg\n",
    "---\n",
    "name: takingnotes2\n",
    "---\n",
    "{glue:text}`blank_caption` \n",
    "```\n",
    "{numref}`Abbildung %s: <takingnotes2>` Zählen von Abfallobjekten am 18.05.2020 am Zürichsee, Richterswil; 3,49 Objekte pro Meter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Schweiz 2020\n",
    "\n",
    "Das IQAASL-Projekt begann im März 2020. Erhebungsorte in den definierten Erhebungsgebieten wurden 2017 beprobt oder neu eingerichtet. Ähnlich wie die Erhebungsergebnisse in der Meeresumwelt sind die Daten über Ufer-Abfallobjekte in der Schweiz sehr unterschiedlich. Die Werte reichen von Null bis zu Tausenden von Objekten und Fragmenten innerhalb von 100 m Distanz entlang von Fluss- oder Seeufer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Das Sammeln von Daten \n",
    "\n",
    "_The guide_ ist die Referenz für dieses Projekt. Die Geographie der Schweiz besteht jedoch nicht aus langen, homogenen Küstenabschnitten. Daher müssen bestimmte Empfehlungen in den Kontext der lokalen Topographie gestellt werden:\n",
    "\n",
    "1. Die empfohlene Länge bleibt bei 100 m, je nach Region ist dies jedoch nicht immer möglich. \n",
    "\n",
    "#### Festlegung des Erhebungsgebiets\n",
    "\n",
    "Ein Vermessungsgebiet wird durch den GPS-Punkt und die aufgezeichneten Vermessungsmasse definiert. Die Mindestbreite ist der Abstand zwischen der Wasserkante und der Uferlinie. In einigen Fällen können die Uferlinie und der hintere Teil des Ufers identisch sein. Für weitere Informationen darüber, wie die Vermessungsflächen gemessen werden, siehe [Das Landnutzungsprofil](luseprofile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "third_subsection_title = Paragraph(\"Festlegung des Erhebungsgebiets\", subsection_title)\n",
    "\n",
    "p_eight = [\n",
    "    \"Ein Vermessungsgebiet wird durch den GPS-Punkt und die aufgezeichneten Vermessungsmasse definiert. \",\n",
    "    \"Die Mindestbreite ist der Abstand zwischen der Wasserkante und der Uferlinie. In einigen Fällen \",\n",
    "    \"können die Uferlinie und der hintere Teil des Ufers identisch sein. Für weitere Informationen \",\n",
    "    'darüber, wie die Vermessungsflächen gemessen werden, siehe ',\n",
    "    '<a href=\"https://hammerdirt-analyst.github.io/IQAASL-End-0f-Sampling-2021/land_use_correlation.html\" color=\"blue\">Landnutzungsprofil</a>. ',\n",
    "    \"<b>Unten:</b> <i>Die Datenerhebungen finden in der Stadt, auf dem Land und in der Agglomeration statt.</i>\",\n",
    "]\n",
    "\n",
    "p_eight = makeAParagraph(p_eight)\n",
    "\n",
    "figure_kwargs = {\n",
    "    \"image_file\":\"resources/images/baselines/tightquarterswholensee.jpg\",\n",
    "    \"caption\": None, \n",
    "    \"original_width\":115.14,\n",
    "    \"original_height\":53.05,\n",
    "    \"desired_width\": 7.95,\n",
    "    \"caption_height\":1,\n",
    "    \"hAlign\": \"CENTER\",\n",
    "}\n",
    "\n",
    "one_of_4 = figureAndCaptionTable(**figure_kwargs)\n",
    "cap_one = Paragraph(\"Kalnach, Aare: ländliches Erholungsgebiet\", caption_style)\n",
    "\n",
    "figure_kwargs.update({\n",
    "    \"image_file\":\"resources/images/baselines/deltabad.jpg\",\n",
    "    \"original_width\":142.2,\n",
    "    \"original_height\":65.6,\n",
    "    \"desired_width\": 7.95,\n",
    "})\n",
    "\n",
    "two_of_4 = figureAndCaptionTable(**figure_kwargs)\n",
    "cap_two = Paragraph(\"Spiez, Thunersee: Vorstadt-Erholungsgebiet\", caption_style)\n",
    "\n",
    "figure_kwargs.update({\n",
    "    \"image_file\":\"resources/images/baselines/lcherz.jpg\",\n",
    "    \"original_width\":142.2,\n",
    "    \"original_height\":65.6,\n",
    "    \"desired_width\": 7.95,\n",
    "})\n",
    "\n",
    "three_of_4 = figureAndCaptionTable(**figure_kwargs)\n",
    "cap_three = Paragraph(\"Lüscherz, Bielersee: ländlicher See\", caption_style)\n",
    "figure_kwargs.update({\n",
    "    \"image_file\":\"resources/images/baselines/snow.jpg\",\n",
    "    \"original_width\":142.2,\n",
    "    \"original_height\":65.6,\n",
    "    \"desired_width\": 7.95,\n",
    "})\n",
    "\n",
    "four_of_4 = figureAndCaptionTable(**figure_kwargs)\n",
    "cap_four = Paragraph(\"Richterswil, Zurichsee: Stadtwanderweg\", caption_style)\n",
    "\n",
    "table_data = [\n",
    "    [[cap_one, one_of_4], [cap_two, two_of_4]],\n",
    "    [[cap_three, three_of_4], [cap_four,four_of_4]]\n",
    "]\n",
    "\n",
    "col_widths = [8*cm, 8*cm]\n",
    "\n",
    "fig_one_to_four = Table(table_data, colWidths=col_widths)\n",
    "\n",
    "new_components = [\n",
    "    PageBreak(),\n",
    "    third_subsection_title,\n",
    "    small_space,\n",
    "    p_eight,\n",
    "    smallest_space,    \n",
    "    fig_one_to_four,\n",
    "  \n",
    "]\n",
    "\n",
    "pdfcomponents = addToDoc(new_components, pdfcomponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# read images\n",
    "img_a = mpimg.imread(\"resources/images/baselines/tightquarterswholensee.jpg\")\n",
    "img_b = mpimg.imread(\"resources/images/baselines/deltabad.jpg\")\n",
    "img_c = mpimg.imread(\"resources/images/baselines/lcherz.jpg\")\n",
    "img_d = mpimg.imread(\"resources/images/baselines/snow.jpg\")\n",
    "\n",
    "# display images\n",
    "fig, ax = plt.subplots(2,2, figsize=(12,8))\n",
    "\n",
    "axone=ax[0,0]\n",
    "sut.hide_spines_ticks_grids(axone)\n",
    "axone.imshow(img_a);\n",
    "axone.set_title(\"Kalnach, Aare: ländliches Erholungsgebiet\", **ck.title_k14)\n",
    "\n",
    "axtwo=ax[0,1]\n",
    "sut.hide_spines_ticks_grids(axtwo)\n",
    "axtwo.imshow(img_b);\n",
    "axtwo.set_title(\"Spiez, Thunersee: Vorstadt-Erholungsgebiet\", **ck.title_k14)\n",
    "\n",
    "axthree=ax[1,0]\n",
    "sut.hide_spines_ticks_grids(axthree)\n",
    "axthree.imshow(img_c);\n",
    "axthree.set_title(\"Lüscherz, Bielersee: ländlicher See\", **ck.title_k14)\n",
    "\n",
    "axfour=ax[1,1]\n",
    "sut.hide_spines_ticks_grids(axfour)\n",
    "axfour.set_title(\"Richterswil, Zurichsee: Stadtwanderweg\", **ck.title_k14)\n",
    "axfour.imshow(img_d);\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "figure_name = f\"types_survey_locations\"\n",
    "types_survey_locations_file_name = f'{save_fig_prefix}{figure_name}.jpeg'\n",
    "\n",
    "# figure caption\n",
    "sample_total_notes = [\n",
    "    \"Erhebungsergebnisse und zusammenfassende Statistiken: \",\n",
    "    \"Proben grösser als 10m und ohne Objekte kleiner als 2,5cm und Chemikalien, n=372\"\n",
    "]\n",
    "\n",
    "types_survey_locations_notes = ''.join(sample_total_notes)\n",
    "\n",
    "glue(\"types_survey_locations_notes\", types_survey_locations_notes, display=False)\n",
    "\n",
    "\n",
    "glue(\"types_survey_locations\", fig, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{glue:figure} types_survey_locations\n",
    "---\n",
    "name: types_survey_locations\n",
    "---\n",
    "{glue:text}`blank_caption` \n",
    "```\n",
    "{numref}`Abbildung {number}: <types_survey_locations>` Die Datenerhebungen finden in der Stadt, auf dem Land und in der Agglomeration statt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Die Länge und Breite des Erhebungsgebiets wird bei jeder Datenerhebung gemessen. So kann die Anzahl der Objekte in einer Standardeinheit unabhängig von den Erhebungsorten angegeben werden. In diesem Bericht wird die von der EU empfohlene Standard-Berichtseinheit verwendet: Abfallobjekte pro 100 Meter.\n",
    "\n",
    "#### Zählen von Objekten \n",
    "\n",
    "Alle sichtbaren Objekte innerhalb eines Untersuchungsgebiets werden gesammelt, klassifiziert und gezählt. Das Gesamtgewicht von Material und Plastik wird ebenfalls erfasst. Die Objekte werden anhand von [ Code-Definitionen ](codegroups) klassifiziert, die auf einer Masterliste von Codes in the guide basieren. Spezielle Objekte, die von lokalem Interesse sind, wurden unter G9xx und G7xx hinzugefügt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "p_nine = [\n",
    "    \"Die Länge und Breite des Erhebungsgebiets wird bei jeder Datenerhebung gemessen. \",\n",
    "    \"So kann die Anzahl der Objekte in einer Standardeinheit unabhängig von den Erhebungsorten \",\n",
    "    \"angegeben werden. In diesem Bericht wird die von der EU empfohlene Standard-Berichtseinheit \",\n",
    "    \"verwendet: Abfallobjekte pro 100 Meter.\"\n",
    "]\n",
    "\n",
    "p_nine = makeAParagraph(p_nine)\n",
    "\n",
    "subsection_four_title = Paragraph(\"Zählen von Objekten\", subsection_title)\n",
    "\n",
    "p_ten = [\n",
    "    \"Alle sichtbaren Objekte innerhalb eines Untersuchungsgebiets werden gesammelt, \",\n",
    "    \"klassifiziert und gezählt. Das Gesamtgewicht von Material und Plastik wird \",\n",
    "    \"ebenfalls erfasst. Die Objekte werden anhand von \",\n",
    "    '<a href=\"https://hammerdirt-analyst.github.io/IQAASL-End-0f-Sampling-2021/code_groups.html\" color=\"blue\">Code-Definitionen klassifiziert</a>, ',\n",
    "    \"die auf einer Masterliste von Codes in the guide basieren. Spezielle Objekte, die von lokalem Interesse sind, wurden unter G9xx und G7xx hinzugefügt.\"\n",
    "]\n",
    "\n",
    "p_ten = makeAParagraph(p_ten)\n",
    "\n",
    "new_components = [\n",
    "    smallest_space,\n",
    "    p_nine,\n",
    "    smallest_space,\n",
    "    subsection_four_title,\n",
    "    small_space,\n",
    "    p_ten,\n",
    "   \n",
    "]\n",
    "\n",
    "pdfcomponents = addToDoc(new_components, pdfcomponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{figure} resources/images/baselines/takingnotes.jpg\n",
    "---\n",
    "name: takingnotes\n",
    "---\n",
    "{glue:text}`blank_caption` \n",
    "```\n",
    "{numref}`Abbildung %s: <takingnotes>` Zählen und Klassifizieren einer Probe. Die Objekte werden nach dem Sammeln sortiert und gezählt. Die ursprüngliche Zählung wird in einem Notizbuch festgehalten, und die Daten werden in die Anwendung [ plages-propres ](https://www.plagespropres.ch/) eingegeben, wenn der Erheber es wünscht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Berechnung der Basislinien \n",
    "\n",
    "Die in den Abschnitten 3 und 4 von A European Threshold Value and Assessment Method for Macro litter on Coastlines und den Abschnitten 6, 7 und 8 von Analysis of a pan-European 2012-2016 beach litter data set beschriebenen Methoden werden auf die Ergebnisse der vorliegenden Untersuchung angewendet.\n",
    "\n",
    "Die verschiedenen Optionen für die Berechnung von Basislinien, die Bestimmung von Konfidenzintervallen und die Identifizierung von Extremwerten werden erläutert und mit Beispielen versehen.\n",
    "\n",
    "__Annahmen:__\n",
    "\n",
    "* Je mehr Abfallobjekte auf dem Boden liegen, desto grösser ist die Wahrscheinlichkeit, dass eine Person sie findet.\n",
    "* Die Erhebungsergebnisse stellen die Mindestmenge an Abfallobjekten an diesem Ort dar.\n",
    "* Für jede Datenerhebung: Das Auffinden eines Objekts hat keinen Einfluss auf die Wahrscheinlichkeit, ein weiteres zu finden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "section_two_title = Paragraph(\"Berechnung der Basislinien\", section_title)\n",
    "\n",
    "p_11 = [\n",
    "    \"Die in den Abschnitten 3 und 4 von A European Threshold Value and Assessment Method \",\n",
    "    \"for Macro litter on Coastlines und den Abschnitten 6, 7 und 8 von Analysis of a \", \n",
    "    \"pan-European 2012-2016 beach litter data set beschriebenen Methoden werden auf die \",\n",
    "    \"Ergebnisse der vorliegenden Untersuchung angewendet.\"\n",
    "]\n",
    "\n",
    "p_11 = makeAParagraph(p_11)\n",
    "\n",
    "p_12 = [\n",
    "    \"Die verschiedenen Optionen für die Berechnung von Basislinien, die Bestimmung von \",\n",
    "    \"Konfidenzintervallen und die Identifizierung von Extremwerten werden erläutert und \",\n",
    "    \"mit Beispielen versehen.\"\n",
    "]\n",
    "\n",
    "p_12 = makeAParagraph(p_12)\n",
    "\n",
    "first_list =[\n",
    "    \"Je mehr Abfallobjekte auf dem Boden liegen, desto grösser ist die Wahrscheinlichkeit, dass eine Person sie findet.\",\n",
    "    \"Die Erhebungsergebnisse stellen die Mindestmenge an Abfallobjekten an diesem Ort dar.\",\n",
    "    \"Für jede Datenerhebung: Das Auffinden eines Objekts hat keinen Einfluss auf die Wahrscheinlichkeit, ein weiteres zu finden.\"\n",
    "]\n",
    "\n",
    "figure_caption = [\n",
    "    '<b>Rechts:</b> Zählen und Klassifizieren einer Probe. Die Objekte werden nach dem Sammeln sortiert und gezählt. ',\n",
    "    \"Die ursprüngliche Zählung wird in einem Notizbuch festgehalten, und die Daten werden in die \",\n",
    "    'Anwendung <a href=\"https://plagespropres.ch\" color=\"blue\"> plagespropres.ch </a> eingegeben, wenn der Erheber es wünscht.'\n",
    "]\n",
    "\n",
    "figure_caption = makeAParagraph(figure_caption, style=caption_style)\n",
    "\n",
    "figure_kwargs.update({\n",
    "    \"image_file\":\"resources/images/baselines/takingnotes.jpg\",\n",
    "    \"original_width\":16.05,\n",
    "    \"original_height\":12.92,\n",
    "    \"desired_width\": 8,\n",
    "    \"caption\": None,\n",
    "})\n",
    "\n",
    "figure_six = figureAndCaptionTable(**figure_kwargs)\n",
    "\n",
    "\n",
    "colWidths=[8.2*cm, 8.2*cm]\n",
    "\n",
    "table_data = [\n",
    "    [[section_two_title, large_space, figure_caption, smallest_space, p_11], figure_six]\n",
    "]\n",
    "figure_six_title = Table(table_data, style=featuredata.side_by_side_style_figure_right, colWidths=colWidths)\n",
    "\n",
    "the_list_title = Paragraph(\"Annahmen:\", bold_block)\n",
    "the_first_list = makeAList(first_list)\n",
    "\n",
    "new_components = [    \n",
    "    small_space,\n",
    "    figure_six_title,\n",
    "    smallest_space,\n",
    "    p_12,\n",
    "    smallest_space,\n",
    "    the_list_title,\n",
    "    the_first_list,      \n",
    "]\n",
    "\n",
    "pdfcomponents = addToDoc(new_components, pdfcomponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Die Daten \n",
    "\n",
    "Nur Datenerhebungen mit einer Länge von mehr als zehn Metern und weniger als 100 Metern werden in die Berechnung der Basislinie einbezogen. Die folgenden Objekte wurden ausgeschlossen:\n",
    "\n",
    "1. Objekte kleiner als 2,5 cm \n",
    "2. Paraffin, Wachs, Öl und andere Chemikalien "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# define the final survey data set here:\n",
    "a_data = survey_data.copy()\n",
    "\n",
    "a_data = a_data[a_data.river_bassin != 'les-alpes']\n",
    "\n",
    "# make a loc_date column from the survey data\n",
    "# before converting to timestamp\n",
    "a_data['loc_date']=tuple(zip(a_data.location, a_data.date))\n",
    "\n",
    "# convert string dates from .csv to timestamp\n",
    "a_data['date']=pd.to_datetime(a_data['date'], format='%Y-%m-%d')\n",
    "\n",
    "# slice by start - end date\n",
    "a_data = a_data[(a_data.date >= start_date)&(a_data.date <= end_date)]\n",
    "\n",
    "# combine lugano and maggiore\n",
    "# if the river bassin name does not equal tresa leave it, else change it to ticino\n",
    "a_data['river_bassin'] = a_data.river_bassin.where(a_data.river_bassin != 'tresa', 'ticino' )\n",
    "\n",
    "# assign the reporting value\n",
    "a_data[unit_label] = (a_data.pcs_m * reporting_unit).round(2)\n",
    "\n",
    "# save the data before aggregating to test\n",
    "before_agg = a_data.copy()\n",
    "\n",
    "# !! Remove the objects less than 2.5cm and chemicals !!\n",
    "codes_todrop = ['G81', 'G78', 'G212', 'G213', 'G214']\n",
    "a_data = a_data[~a_data.code.isin(codes_todrop)]\n",
    "\n",
    "# use the code groups to get rid of all objects less than 5mm\n",
    "a_data = a_data[a_data.groupname !=  'micro plastics (< 5mm)']\n",
    "\n",
    "# match records to survey data\n",
    "fd_dims= dfDims[(dfDims.location.isin(a_data.location.unique()))&(dfDims.date >= start_date)&(dfDims.date <= end_date)].copy()\n",
    "\n",
    "# make a loc_date column and get the unique values\n",
    "fd_dims['loc_date'] = list(zip(fd_dims.location, fd_dims.date))\n",
    "\n",
    "# map the survey area name to the dims data record\n",
    "a_map = fd_dims[['loc_date', 'area']].set_index('loc_date')\n",
    "l_map = fd_dims[['loc_date', 'length']].set_index('loc_date')\n",
    "\n",
    "# map length and area from dims to survey data\n",
    "for a_survey in fd_dims.loc_date.unique():\n",
    "    a_data.loc[a_data.loc_date == a_survey, 'length'] = l_map.loc[[a_survey], 'length'][0]\n",
    "    a_data.loc[a_data.loc_date == a_survey, 'area'] = a_map.loc[[a_survey], 'area'][0]\n",
    "\n",
    "# exclude surveys less 10 meters or less\n",
    "gten_lhun = a_data.loc[(a_data.length > 10)].copy()\n",
    "\n",
    "# this is a common aggregation\n",
    "agg_pcs_quantity = {unit_label:'sum', 'quantity':'sum'}\n",
    "\n",
    "# survey totals by location\n",
    "dt_all = gten_lhun.groupby(['loc_date','location','river_bassin', 'water_name_slug','date'], as_index=False).agg(agg_pcs_quantity)\n",
    "\n",
    "# palettes and labels\n",
    "bassin_pallette = {'rhone':'dimgray', 'aare':'salmon', 'linth':'tan', 'ticino':'steelblue', 'reuss':'purple'}\n",
    "comp_labels = {\"linth\":\"Linth/Limmat\", \"rhone\":\"Rhône\", 'aare':\"Aare\", \"ticino\":\"Ticino/Cerisio\", \"reuss\":\"Reuss\"}\n",
    "comp_palette = {\"Linth/Limmat\":\"dimgray\", \"Rhône\":\"tan\", \"Aare\":\"salmon\", \"Ticino/Cerisio\":\"steelblue\", \"Reuss\":\"purple\"}\n",
    "\n",
    "# months locator, can be confusing\n",
    "# https://matplotlib.org/stable/api/dates_api.html\n",
    "months = mdates.MonthLocator(interval=1)\n",
    "months_fmt = mdates.DateFormatter('%b')\n",
    "days = mdates.DayLocator(interval=7)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "gs = GridSpec(1,8)\n",
    "\n",
    "ax = fig.add_subplot(gs[:,0:4])\n",
    "axtwo = fig.add_subplot(gs[:, 4:])\n",
    "\n",
    "# scale the chart as needed to accomodate for extreme values\n",
    "scale_back = 98\n",
    "\n",
    "# the results gets applied to the y_limit function in the chart\n",
    "the_90th = np.percentile(dt_all[unit_label], scale_back)\n",
    "\n",
    "# the survey totals\n",
    "sns.scatterplot(data=dt_all, x='date', y=unit_label, hue='river_bassin', palette=bassin_pallette, alpha=1, ax=ax)\n",
    "\n",
    "# set params on ax:\n",
    "ax.set_ylim(0,the_90th )\n",
    "ax.set_ylabel(unit_label, **ck.xlab_k14)\n",
    "\n",
    "ax.set_xlabel(\"\")\n",
    "ax.xaxis.set_minor_locator(days)\n",
    "ax.xaxis.set_major_formatter(months_fmt)\n",
    "h,l = ax.get_legend_handles_labels()\n",
    "\n",
    "ax.legend(h, l)\n",
    "\n",
    "# axtwo\n",
    "a_color = \"dodgerblue\"\n",
    "\n",
    "# get the basic statistics from pd.describe\n",
    "cs = dt_all[unit_label].describe().round(2)\n",
    "\n",
    "# change the names\n",
    "csx = sut.change_series_index_labels(cs, featuredata.createSummaryTableIndex(unit_label, language=\"de\"))\n",
    "# combined_summary = csx.apply(lambda x: featuredata.thousandsSeparator(int(x)))\n",
    "\n",
    "\n",
    "# csx.loc[\"max p/100 m \"] = featuredata.thousandsSeparator(csx.loc[\"max p/100 m\"])\n",
    "\n",
    "combined_summary = [(x, featuredata.thousandsSeparator(int(csx.loc[x]))) for x in csx.index]\n",
    "\n",
    "sut.hide_spines_ticks_grids(axtwo)\n",
    "\n",
    "a_table = sut.make_a_table(axtwo, combined_summary,  colLabels=[\"Stat\", unit_label], colWidths=[.6,.4], bbox=[0, 0, 1, 1])\n",
    "a_table.get_celld()[(0,0)].get_text().set_text(\" \")\n",
    "\n",
    "axtwo.tick_params(which='both', axis='both', labelsize=14)\n",
    "ax.tick_params(which='both', axis='both', labelsize=14)\n",
    "ax.grid(visible=True, which='major', axis='y', linestyle='-', linewidth=1, c='black', alpha=.1, zorder=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "figure_name = f\"baseline_sample_totals\"\n",
    "sample_totals_file_name = f'{save_fig_prefix}{figure_name}.jpeg'\n",
    "save_figure_kwargs.update({\"fname\":sample_totals_file_name})\n",
    "plt.savefig(**save_figure_kwargs)\n",
    "\n",
    "# figure caption\n",
    "sample_total_notes = [\n",
    "    \"Erhebungsergebnisse und zusammenfassende Statistiken: \",\n",
    "    \"Proben grösser als 10m und ohne Objekte kleiner als 2,5cm und Chemikalien, n=372\"\n",
    "]\n",
    "\n",
    "sample_total_notes = ''.join(sample_total_notes)\n",
    "\n",
    "glue(\"baseline_sample_total_notes\", sample_total_notes, display=False)\n",
    "\n",
    "\n",
    "glue(\"baseline_sample_totals\", fig, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{glue:figure} baseline_sample_totals\n",
    "---\n",
    "name: \"baseline_sample_totals\"\n",
    "---\n",
    "{glue:text}`blank_caption` \n",
    "```\n",
    "{numref}`Abbildung {number}: <baseline_sample_totals>` {glue:text}`baseline_sample_total_notes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "sub_section_one_title = Paragraph(\"Die Daten\", style=subsection_title)\n",
    "\n",
    "p_13 = [\n",
    "    \"Nur Datenerhebungen mit einer Länge von mehr als zehn Metern und weniger als 100 Metern \",\n",
    "    \"werden in die Berechnung der Basislinie einbezogen. Die folgenden Objekte wurden ausgeschlossen:\"\n",
    "]\n",
    "\n",
    "p_13 = makeAParagraph(p_13)\n",
    "\n",
    "data_d_list = [\n",
    "    \"Objekte kleiner als 2,5 cm\",\n",
    "    \"Paraffin, Wachs, Öl und andere Chemikalien\"\n",
    "]\n",
    "\n",
    "baseline_list_one = makeAList(data_d_list)    \n",
    "\n",
    "fig_7_cap = Paragraph(sample_total_notes, style=caption_style)\n",
    "\n",
    "figure_kwargs.update({\n",
    "    \"image_file\":sample_totals_file_name,\n",
    "    \"original_width\":25.4,\n",
    "    \"original_height\":12.7,\n",
    "    \"desired_width\": 12,\n",
    "    \"caption\": fig_7_cap,\n",
    "})\n",
    "\n",
    "figure_seven = figureAndCaptionTable(**figure_kwargs)\n",
    "\n",
    "new_components = [\n",
    "    small_space,\n",
    "    sub_section_one_title,\n",
    "    small_space,\n",
    "    p_13,\n",
    "    smallest_space,\n",
    "    baseline_list_one,\n",
    "    small_space,    \n",
    "    figure_seven,\n",
    "    smallest_space,\n",
    "  \n",
    "]\n",
    "pdfcomponents = addToDoc(new_components, pdfcomponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# percentile rankings  1, 5, 10, 15, 20\n",
    "these_vals = []\n",
    "for element in [.01,.05,.10,.15,.20, .5, .9 ]:\n",
    "    a_val = np.quantile(dt_all[unit_label].to_numpy(), element)\n",
    "    these_vals.append((F\"{int(element*100)}.\",F\"{int(a_val)}\"))\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "gs = GridSpec(1,5)\n",
    "\n",
    "ax = fig.add_subplot(gs[:,0:3])\n",
    "axtwo = fig.add_subplot(gs[:, 3:])\n",
    "ax.grid(visible=True, which='major', axis='y', linestyle='-', linewidth=1, c='black', alpha=.1, zorder=0)\n",
    "\n",
    "sns.histplot(data=dt_all, x=unit_label, stat='count', ax=ax, alpha=0.6)\n",
    "ax.axvline(x=dt_all[unit_label].median(), c='magenta', label='Median')\n",
    "ax.axvline(x=dt_all[unit_label].mean(), c='red', label='Durchschnitt')\n",
    "ax.legend()\n",
    "\n",
    "sut.hide_spines_ticks_grids(axtwo)\n",
    "\n",
    "\n",
    "table_two = sut.make_a_table(axtwo, these_vals, colLabels=('ranking', unit_label), colWidths=[.5,.5], bbox=[0, 0, 1, 1])\n",
    "table_two.get_celld()[(0,0)].get_text().set_text(\"ranking\")\n",
    "table_two.set_fontsize(14)\n",
    "\n",
    "axtwo.tick_params(which='both', axis='both', labelsize=14)\n",
    "ax.tick_params(which='both', axis='both', labelsize=14)\n",
    "\n",
    "figure_name = f\"empirical_dist_rankings_baseline\"\n",
    "empirical_dist_file_name = f'{save_fig_prefix}{figure_name}.jpeg'\n",
    "save_figure_kwargs.update({\"fname\":empirical_dist_file_name})\n",
    "plt.savefig(**save_figure_kwargs)\n",
    "\n",
    "# figure caption\n",
    "empirical_dist_notes = [\n",
    "    \"Verteilung der Datenerhebungen und Perzentilwerte: alle Erhebungen. Beachten Sie, dass der \",\n",
    "    \"Mittelwert (341p/100m) grösser ist als der Median (180p/100m).\"\n",
    "]\n",
    "\n",
    "empirical_dist_notes = ''.join(empirical_dist_notes)\n",
    "\n",
    "glue(\"empirical_dist_rankings_baseline_notes\", empirical_dist_notes, display=False)\n",
    "\n",
    "\n",
    "glue(\"empirical_dist_rankings_baseline\", fig, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{glue:figure} empirical_dist_rankings_baseline\n",
    ":name: \"empirical_dist_rankings_baseline\"\n",
    "\n",
    "\n",
    "{glue:text}`blank_caption` \n",
    "\n",
    "```\n",
    "\n",
    "{numref}`Abbildung {number}: <empirical_dist_rankings_baseline>` {glue:text}`empirical_dist_rankings_baseline_notes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Die Bewertungsmetrik \n",
    "\n",
    "Die Berechnung von Basiswerten erfordert die Aggregation von Erhebungsergebnissen auf verschiedenen zeitlichen und geografischen Ebenen. Das ist die beste Methode: \n",
    "\n",
    "* Robust in Bezug auf Ausreisser\n",
    "* Einfach zu berechnen\n",
    "* Weithin verstanden\n",
    "\n",
    "Die beiden gebräuchlichsten Teststatistiken, die zum Vergleich von Daten verwendet werden, sind der Mittelwert und der Median. Der Mittelwert ist der beste Prädiktor für die zentrale Tendenz, wenn die Daten ungefähr normal verteilt sind. Die Ergebnisse der Untersuchungen von Strand-Abfallaufkommen weisen jedoch eine hohe Varianz im Verhältnis zum Mittelwert auf. Es können Methoden auf die Daten angewendet werden, um die Auswirkungen der hohen Varianz bei der Berechnung des Mittelwerts zu verringern: \n",
    "\n",
    "1. _getrimmter Mittelwert:_ entfernt einen kleinen, festgelegten Prozentsatz der grössten und kleinsten Werte, bevor der Mittelwert berechnet wird \n",
    "2. _tri-Mittelwert:_ der gewichtete Durchschnitt des Medians und des oberen und unteren Quartils $(Q1 + 2Q2 + Q3)/4$\n",
    "3. _mittleres Scharnier:_ $(Q1 + Q3)/2$\n",
    "\n",
    "Die bisherigen Methoden sind zwar wirksam, um die Auswirkungen von Ausreissern zu reduzieren, aber sie sind nicht so einfach zu berechnen wie der Mittelwert oder der Median, so dass die Signifikanz der Ergebnisse möglicherweise nicht richtig verstanden wird.\n",
    "\n",
    "Der Median (50. Perzentil) ist ein ebenso guter Prädiktor für die zentrale Tendenz, wird aber im Vergleich zum Mittelwert viel weniger von Extremwerten beeinflusst. Je mehr sich ein Datensatz einer Normalverteilung nähert, desto näher kommen sich Median und Mittelwert. Der Median und die dazugehörigen Perzentil-Funktionen sind in den meisten Tabellenkalkulationsprogrammen verfügbar.\n",
    "\n",
    "Aus den oben genannten Gründen wird der Medianwert einer Mindestanzahl von Proben, die während eines Probenahmezeitraums in einem Erhebungsgebiet gesammelt wurden, als statistisch geeignet für die Bewertung von Strand-Abfallobjekten angesehen. Für die Meeresumwelt beträgt die Mindestanzahl der Proben 40 pro Unterregion und der Probenahmezeitraum 6 Jahre. {cite}`eubaselines` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig_8_cap = Paragraph(empirical_dist_notes, style=caption_style)\n",
    "\n",
    "figure_kwargs.update({\n",
    "    \"image_file\":empirical_dist_file_name,\n",
    "    \"original_width\":25.4,\n",
    "    \"original_height\":12.7,\n",
    "    \"desired_width\": 12,\n",
    "    \"caption\": fig_8_cap,\n",
    "})\n",
    "\n",
    "figure_eight = figureAndCaptionTable(**figure_kwargs)\n",
    "\n",
    "subsection_two_title = Paragraph(\"Die Bewertungsmetrik\", subsection_title)\n",
    "p_14 = [\n",
    "    \"Die Berechnung von Basiswerten erfordert die Aggregation von Erhebungsergebnissen auf verschiedenen z\",\n",
    "    \"eitlichen und geografischen Ebenen. Das ist die beste Methode:\"\n",
    "]\n",
    "\n",
    "p_14 = makeAParagraph(p_14)\n",
    "\n",
    "baseline_list_three = [\n",
    "    \"Robust in Bezug auf Ausreisser\",\n",
    "    \"Einfach zu berechnen\",\n",
    "    \"Weithin verstanden\"\n",
    "]\n",
    "\n",
    "qualities = makeAList(baseline_list_three)\n",
    "\n",
    "p_15 = [\n",
    "    \"Die beiden gebräuchlichsten Teststatistiken, die zum Vergleich von Daten \",\n",
    "    \"verwendet werden, sind der Mittelwert und der Median. Der Mittelwert ist der \",\n",
    "    \"beste Prädiktor für die zentrale Tendenz, wenn die Daten ungefähr normal verteilt \",\n",
    "    \"sind. Die Ergebnisse der Untersuchungen von Strand-Abfallaufkommen weisen jedoch \",\n",
    "    \"eine hohe Varianz im Verhältnis zum Mittelwert auf. Es können Methoden auf die Daten \",\n",
    "    \"angewendet werden, um die Auswirkungen der hohen Varianz bei der Berechnung des \",\n",
    "    \"Mittelwerts zu verringern:\"\n",
    "]\n",
    "\n",
    "p_15 = makeAParagraph(p_15)\n",
    "\n",
    "baseline_list_four = [\n",
    "    \"<i>getrimmter Mittelwert:</i> entfernt einen kleinen, festgelegten Prozentsatz der grössten und kleinsten Werte, bevor der Mittelwert berechnet wird\",\n",
    "    \"<i>tri-Mittelwert: </i> der gewichtete Durchschnitt des Medians und des oberen und unteren Quartils (Q1 + 2Q2 + Q3)/4\",\n",
    "    \"<i>Mittelscharnier: (Q1 + Q3)/2</i>\",\n",
    "]\n",
    "\n",
    "methods = makeAList(baseline_list_four)\n",
    "\n",
    "p_16 = [\n",
    "    \"Die bisherigen Methoden sind zwar wirksam, um die Auswirkungen von Ausreissern zu reduzieren, \",\n",
    "    \"aber sie sind nicht so einfach zu berechnen wie der Mittelwert oder der Median, so dass die \",\n",
    "    \"Signifikanz der Ergebnisse möglicherweise nicht richtig verstanden wird.\"\n",
    "]\n",
    "\n",
    "p_17 = [\n",
    "    \"Der Median (50. Perzentil) ist ein ebenso guter Prädiktor für die zentrale Tendenz, wird aber im \",\n",
    "    \"Vergleich zum Mittelwert viel weniger von Extremwerten beeinflusst. Je mehr sich ein Datensatz \",\n",
    "    \"einer Normalverteilung nähert, desto näher kommen sich Median und Mittelwert. Der Median und die \",\n",
    "    \"dazugehörigen Perzentil-Funktionen sind in den meisten Tabellenkalkulationsprogrammen verfügbar.\"\n",
    "]\n",
    "\n",
    "p_18 = [\n",
    "    \"Aus den oben genannten Gründen wird der Medianwert einer Mindestanzahl von Proben, die während \",\n",
    "    \"eines Probenahmezeitraums in einem Erhebungsgebiet gesammelt wurden, als statistisch geeignet \",\n",
    "    \"für die Bewertung von Strand-Abfallobjekten angesehen. Für die Meeresumwelt beträgt die Mindestanzahl \",\n",
    "    'der Proben 40 pro Unterregion und der Probenahmezeitraum 6 Jahre.<a href=\"#HG19\" color=\"blue\">(HG19)</a>'\n",
    "]\n",
    "\n",
    "p_16_to_18 = sectionParagraphs([p_16, p_17, p_18], smallspace=smallest_space)\n",
    "\n",
    "new_components = [\n",
    "    figure_eight,\n",
    "    smallest_space,\n",
    "    subsection_two_title,\n",
    "    small_space,\n",
    "    p_14,\n",
    "    smallest_space,\n",
    "    qualities,\n",
    "    smallest_space,\n",
    "    p_15,\n",
    "    smallest_space,\n",
    "    methods,\n",
    "    *p_16_to_18,  \n",
    "]\n",
    "\n",
    "pdfcomponents = addToDoc(new_components, pdfcomponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Konfidenzintervalle (KIs) \n",
    "\n",
    "Konfidenzintervalle (KIs) helfen dabei, die Unsicherheit der Ergebnisse von Strand-Abfallobjekten im Hinblick auf allgemeine Schlussfolgerungen über die Häufigkeit von Strand-Abfallobjekten in einer Region zu vermitteln. Das KI gibt den unteren und oberen Bereich der Schätzung der Teststatistik angesichts der Stichprobendaten an. \n",
    "\n",
    "Der beste Weg, die Unsicherheit zu verringern, ist eine angemessene Anzahl von Proben für die Region oder das Gebiet von Interesse zu erheben. Strand-Abfallerhebungen weisen jedoch eine hohe Varianz auf und jede Schätzung eines Gesamtwerts sollte diese Varianz oder Unsicherheit widerspiegeln. kIs geben einen wahrscheinlichen Wertebereich angesichts der Unsicherheit/Varianz der Daten an.{cite}`eubaselines`\n",
    "\n",
    "Bei dieser Methode werden die Daten NICHT von den Basisberechnungen und Konfidenzintervallen ausgeschlossen.\n",
    "\n",
    "> Man einigte sich darauf, die extremen Ergebnisse im Datensatz zu belassen, gleichzeitig aber die Notwendigkeit zu betonen, extreme Daten von Fall zu Fall zu überprüfen und den Median für die Berechnung von Durchschnittswerten zu verwenden. Auf diese Weise können alle Daten verwendet werden, ohne dass die Ergebnisse durch einzelne aussergewöhnlich hohe Abfallaufkommen verzerrt werden. {cite}`threshholdeu`\n",
    "\n",
    "#### Bootstrap-Methoden: \n",
    "Bootstrapping ist eine Methode zur Wiederholung von Stichproben, bei der Zufallsstichproben mit Ersetzung verwendet werden, um den Stichprobenprozess zu wiederholen oder zu simulieren. Bootstrapping ermöglicht die Schätzung der Stichprobenverteilung von Stichprobenstatistiken unter Verwendung von Zufallsstichprobenverfahren. {cite}`bootstrapdef` {cite}`bsci` {cite}`usingbootstrap`\n",
    "\n",
    "Bootstrap-Methoden werden verwendet, um das KI der Teststatistiken zu berechnen, indem der Stichprobenprozess wiederholt wird und der Median bei jeder Wiederholung ausgewertet wird. Der Wertebereich, der durch die mittleren 95 % der Bootstrap-Ergebnisse beschrieben wird, ist das KI für die beobachtete Teststatistik. \n",
    "\n",
    "Es stehen mehrere Berechnungsmethoden zur Auswahl, z. B. Perzentil, BCa und Student’s t. Für dieses Beispiel wurden zwei Methoden getestet: \n",
    "\n",
    "1. Perzentil-Bootstrap\n",
    "2. Bias-korrigiertes beschleunigtes Bootstrap-Konfidenzintervall (BCa) \n",
    "\n",
    "Die Perzentil-Methode berücksichtigt nicht die Form der zugrundeliegenden Verteilung, was zu Konfidenzintervallen führen kann, die nicht mit den Daten übereinstimmen. Die Bca-Methode korrigiert dies. Die Implementierung dieser Methoden ist mit den bereits zitierten Paketen einfach zu bewerkstelligen. {cite}`bcatheory` {cite}`bcaimpdrysdale` {cite}`bcaconfidence`\n",
    "\n",
    "### Vergleich der Bootstrap-KIs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "subsection_ki = Paragraph(\"Konfidenzintervalle (KIs)\", subsection_title)\n",
    "\n",
    "p_19 = [\n",
    "    \"Konfidenzintervalle (KIs) helfen dabei, die Unsicherheit der Ergebnisse von \",\n",
    "    \"Strand-Abfallobjekten im Hinblick auf allgemeine Schlussfolgerungen über die \",\n",
    "    \"Häufigkeit von Strand-Abfallobjekten in einer Region zu vermitteln. Das KI gibt \",\n",
    "    \"den unteren und oberen Bereich der Schätzung der Teststatistik angesichts der Stichprobendaten an.\"\n",
    "]\n",
    "\n",
    "p_20 = [\n",
    "    \"Der beste Weg, die Unsicherheit zu verringern, ist eine angemessene Anzahl von Proben für \",\n",
    "    \"die Region oder das Gebiet von Interesse zu erheben. Strand-Abfallerhebungen weisen \",\n",
    "    \"jedoch eine hohe Varianz auf und jede Schätzung eines Gesamtwerts sollte diese Varianz \",\n",
    "    \"oder Unsicherheit widerspiegeln. kIs geben einen wahrscheinlichen Wertebereich angesichts \",\n",
    "    'der Unsicherheit/Varianz der Daten an. <a href=\"#HG19\" color=\"blue\">(HG19)</a>'\n",
    "]\n",
    "\n",
    "p_21 = [\n",
    "    \"Bei dieser Methode werden die Daten NICHT von den Basisberechnungen und Konfidenzintervallen ausgeschlossen.\"\n",
    "]\n",
    "\n",
    "p_22 = [\n",
    "    '\"Man einigte sich darauf, die extremen Ergebnisse im Datensatz zu belassen, gleichzeitig aber die ',\n",
    "    \"Notwendigkeit zu betonen, extreme Daten von Fall zu Fall zu überprüfen und den Median für die Berechnung \",\n",
    "    \"von Durchschnittswerten zu verwenden. Auf diese Weise können alle Daten verwendet werden, ohne dass die \",\n",
    "    'Ergebnisse durch einzelne aussergewöhnlich hohe Abfallaufkommen verzerrt werden\". <a href=\"#VLW20\" color=\"blue\">(VLW20)</a>'\n",
    "]\n",
    "        \n",
    "p_22 = ''.join(p_22)\n",
    "\n",
    "p_22 = Paragraph(p_22, block_quote_style)\n",
    "\n",
    "p_19_to_21 = sectionParagraphs([p_19, p_20, p_21], smallspace=smallest_space)\n",
    "\n",
    "subsection_boot = Paragraph(\"Bootstrap-Methoden:\", subsection_title)\n",
    "\n",
    "p_23 = [\n",
    "    \"Bootstrapping ist eine Methode zur Wiederholung von Stichproben, bei der Zufallsstichproben \",\n",
    "    \"mit Ersetzung verwendet werden, um den Stichprobenprozess zu wiederholen oder zu simulieren. \",\n",
    "    \"Bootstrapping ermöglicht die Schätzung der Stichprobenverteilung von Stichprobenstatistiken unter\",\n",
    "    \"Verwendung von Zufallsstichprobenverfahren.\",\n",
    "    '<a href=\"#Wika\" color=\"blue\">(Wika)</a><a href=\"#JLGC19\" color=\"blue\">(JLGC19)</a><a href=\"#Sta21b\" color=\"blue\">(Sta21b)</a>'\n",
    "]\n",
    "\n",
    "p_24 = [\n",
    "    \"Bootstrap-Methoden werden verwendet, um das KI der Teststatistiken zu berechnen, indem der \",\n",
    "    \"Stichprobenprozess wiederholt wird und der Median bei jeder Wiederholung ausgewertet wird. \",\n",
    "    \"Der Wertebereich, der durch die mittleren 95 % der Bootstrap-Ergebnisse beschrieben wird, \",\n",
    "    \"ist das KI für die beobachtete Teststatistik.\"\n",
    "]\n",
    "\n",
    "p_25 = [\n",
    "    \"Es stehen mehrere Berechnungsmethoden zur Auswahl, z. B. Perzentil, BCa und Student’s t. \",\n",
    "    \"Für dieses Beispiel wurden zwei Methoden getestet:\"\n",
    "]\n",
    "\n",
    "p_23_to_25 = sectionParagraphs([p_23, p_24, p_25], smallspace=smallest_space)\n",
    "\n",
    "p_26 = [\n",
    "    \"Perzentil-Bootstrap\",\n",
    "    \"Bias-korrigiertes beschleunigtes Bootstrap-Konfidenzintervall (Bca)\"\n",
    "]\n",
    "\n",
    "p_26 = makeAList(p_26)\n",
    "\n",
    "\n",
    "p_27 = [\n",
    "    \"Die Perzentil-Methode berücksichtigt nicht die Form der zugrundeliegenden Verteilung, \",\n",
    "    \"was zu Konfidenzintervallen führen kann, die nicht mit den Daten übereinstimmen. Die Bca-Methode \",\n",
    "    \"korrigiert dies. Die Implementierung dieser Methoden ist mit den bereits zitierten Paketen \",\n",
    "    \"einfach zu bewerkstelligen.\",\n",
    "    '<a href=\"#Efr87\" color=\"blue\">(Efr87)</a><a href=\"#dry20\" color=\"blue\">(dry20)</a><a href=\"#Boi19\" color=\"blue\">(Boi19)</a>'\n",
    "]\n",
    "\n",
    "p_27 = makeAParagraph(p_27)\n",
    "    \n",
    "\n",
    "# references\n",
    "wikapub = 'Bootstrapping. wikipedia https://en.wikipedia.org/wiki/Bootstrapping_(statistics).'\n",
    "\n",
    "wika = makeAParagraph(featuredata.makeBibEntry(name=\"Wika\", team=\"Wikipedia\", pub=wikapub))\n",
    "\n",
    "jlgc_team = \"Kwanghee Jung, Jaehoon Lee, Vibhuti Gupta, and Gyeongcheol Cho.\"\n",
    "jlgcpub = [\n",
    "    \"Comparison of bootstrap confidence interval methods for gsca using a monte carlo simulation. \",\n",
    "    \"Frontiers in Psychology, 10:2215, 2019. URL: \",\n",
    "    'https://www.frontiersin.org/article/10.3389/fpsyg.2019.02215, doi:10.3389/fpsyg.2019.02215'\n",
    "]\n",
    "jlgcpub = ''.join(jlgcpub)\n",
    "\n",
    "jlgc19 = makeAParagraph(featuredata.makeBibEntry(name=\"JLGC19\", team=jlgc_team, pub=jlgcpub))\n",
    "\n",
    "\n",
    "sta21bpub = [\n",
    "    \"Is it true that the percentile bootstrap should never be used? 2021. URL: \",\n",
    "    'https://stats.stackexchange.com/questions/355781/is-it-true-that-the-percentile-bootstrap-should-never-be-used'\n",
    "]\n",
    "\n",
    "sta21pub = ''.join(sta21bpub)\n",
    "\n",
    "sta21b = makeAParagraph(featuredata.makeBibEntry(name=\"Sta21b\", team=\"Stackexchange\", pub=sta21pub))\n",
    "\n",
    "efr87pub = [\n",
    "    \"Better bootstrap confidence intervals. Journal of the American \",\n",
    "    \"Statistical Association, 82(397):171–185, 1987. URL: \",\n",
    "    'https://www.tandfonline.com/doi/abs/10.1080/01621459.1987.10478410, doi:10.1080/01621459.1987.10478410'\n",
    "]\n",
    "\n",
    "efr87pub = ''.join(efr87pub)\n",
    "\n",
    "efr87 = makeAParagraph(featuredata.makeBibEntry(name=\"Efr87\", team=\"Bradley Efron\", pub=efr87pub))\n",
    "\n",
    "dry20pub = [\n",
    "    \"Implementing the bias-corrected and \",\n",
    "    \"accelerated bootstrap in python. 2020. URL: \",\n",
    "    'https://www.erikdrysdale.com/bca_python/'\n",
    "]\n",
    "\n",
    "dry20pub = ''.join(dry20pub)\n",
    "\n",
    "dry20 = makeAParagraph(featuredata.makeBibEntry(name=\"dry20\", team=\"Erik Drysdale\", pub=dry20pub))\n",
    "\n",
    "boi19pub = [\n",
    "    \"Introduction to data analysis in the biological sciences. 2019.\",\n",
    "    'http://bebi103.caltech.edu.s3-website-us-east-1.amazonaws.com/2019a/content/index.html#'\n",
    "    \n",
    "]\n",
    "boi19pub = ''.join(boi19pub)\n",
    "\n",
    "boi19 = makeAParagraph(featuredata.makeBibEntry(name=\"Boi19\", team=\"Justin Bois\", pub=boi19pub))\n",
    "\n",
    "references = [*references, smallest_space,wika, smallest_space, jlgc19, smallest_space, sta21b, smallest_space, efr87, smallest_space, dry20, smallest_space, boi19]\n",
    "\n",
    "new_components = [\n",
    "    smallest_space,\n",
    "    subsection_ki,\n",
    "    small_space,\n",
    "    *p_19_to_21,\n",
    "    p_22,\n",
    "    smallest_space,\n",
    "    subsection_boot,\n",
    "    small_space,\n",
    "    *p_23_to_25,\n",
    "    p_26,\n",
    "    smallest_space,\n",
    "    p_27,    \n",
    "   \n",
    "]\n",
    "\n",
    "pdfcomponents = addToDoc(new_components, pdfcomponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# this code was modified from this source:\n",
    "# http://bebi103.caltech.edu.s3-website-us-east-1.amazonaws.com/2019a/content/recitations/bootstrapping.html\n",
    "# if you want to get the confidence interval around another point estimate use np.percentile\n",
    "# and add the percentile value as a parameter\n",
    "\n",
    "def draw_bs_sample(data):\n",
    "    \"\"\"Draw a bootstrap sample from a 1D data set.\"\"\"\n",
    "    return np.random.choice(data, size=len(data))\n",
    "\n",
    "def compute_jackknife_reps(data, statfunction=None, stat_param=False):\n",
    "    '''Returns jackknife resampled replicates for the given data and statistical function'''\n",
    "    # Set up empty array to store jackknife replicates\n",
    "    jack_reps = np.empty(len(data))\n",
    "\n",
    "    # For each observation in the dataset, compute the statistical function on the sample\n",
    "    # with that observation removed\n",
    "    for i in range(len(data)):\n",
    "        jack_sample = np.delete(data, i)\n",
    "        if not stat_param:\n",
    "            jack_reps[i] = statfunction(jack_sample)\n",
    "        else:\n",
    "            jack_reps[i] = statfunction(jack_sample, stat_param)          \n",
    "        \n",
    "    return jack_reps\n",
    "\n",
    "\n",
    "def compute_a(jack_reps):\n",
    "    '''Returns the acceleration constant a'''\n",
    "\n",
    "    mean = np.mean(jack_reps)\n",
    "    try:\n",
    "        a = sum([(x**-(i+1)- (mean**-(i+1)))**3 for i,x in enumerate(jack_reps)])\n",
    "        b = sum([(x**-(i+1)-mean-(i+1))**2 for i,x in enumerate(jack_reps)])\n",
    "        c = 6*(b**(3/2))\n",
    "        data = a/c\n",
    "    except:\n",
    "        print(mean)\n",
    "    return data\n",
    "\n",
    "\n",
    "def bootstrap_replicates(data, n_reps=1000, statfunction=None, stat_param=False):\n",
    "    '''Computes n_reps number of bootstrap replicates for given data and statistical function'''\n",
    "    boot_reps = np.empty(n_reps)\n",
    "    for i in range(n_reps):\n",
    "        if not stat_param:\n",
    "            boot_reps[i] = statfunction(draw_bs_sample(data))\n",
    "        else:\n",
    "            boot_reps[i] = statfunction(draw_bs_sample(data), stat_param)     \n",
    "        \n",
    "    return boot_reps\n",
    "\n",
    "\n",
    "def compute_z0(data, boot_reps, statfunction=None, stat_param=False):\n",
    "    '''Computes z0 for given data and statistical function'''\n",
    "    if not stat_param:\n",
    "        s = statfunction(data)\n",
    "    else:\n",
    "        s = statfunction(data, stat_param)\n",
    "    return stats.norm.ppf(np.sum(boot_reps < s) / len(boot_reps))\n",
    "\n",
    "\n",
    "def compute_bca_ci(data, alpha_level, n_reps=1000, statfunction=None, stat_param=False):\n",
    "    '''Returns BCa confidence interval for given data at given alpha level'''\n",
    "    # Compute bootstrap and jackknife replicates\n",
    "    boot_reps = bootstrap_replicates(data, n_reps, statfunction=statfunction, stat_param=stat_param)\n",
    "    jack_reps = compute_jackknife_reps(data, statfunction=statfunction, stat_param=stat_param)\n",
    "\n",
    "    # Compute a and z0\n",
    "    a = compute_a(jack_reps)\n",
    "    z0 = compute_z0(data, boot_reps, statfunction=statfunction, stat_param=stat_param)\n",
    "\n",
    "    # Compute confidence interval indices\n",
    "    alphas = np.array([alpha_level/2., 1-alpha_level/2.])\n",
    "    zs = z0 + stats.norm.ppf(alphas).reshape(alphas.shape+(1,)*z0.ndim)\n",
    "    avals = stats.norm.cdf(z0 + zs/(1-a*zs))\n",
    "    ints = np.round((len(boot_reps)-1)*avals)\n",
    "    ints = np.nan_to_num(ints).astype('int')\n",
    "\n",
    "    # Compute confidence interval\n",
    "    boot_reps = np.sort(boot_reps)\n",
    "    ci_low = boot_reps[ints[0]]\n",
    "    ci_high = boot_reps[ints[1]]\n",
    "    return (ci_low, ci_high)\n",
    "\n",
    "quantiles = [.1, .2, .5, .9]\n",
    "q_vals = {x:dt_all[unit_label].quantile(x) for x in quantiles}\n",
    "\n",
    "the_bcas = {}\n",
    "for a_rank in quantiles:\n",
    "    an_int = int(a_rank*100)\n",
    "    a_result = compute_bca_ci(dt_all[unit_label].to_numpy(), .05, n_reps=5000, statfunction=np.percentile, stat_param=an_int)\n",
    "    observed = np.percentile(dt_all[unit_label].to_numpy(), an_int)\n",
    "    the_bcas.update({F\"{int(a_rank*100)}\":{'2.5% ci':a_result[0], \"Beobachtung\": observed, '97.5% ci': a_result[1]}})\n",
    "\n",
    "\n",
    "bca_cis = pd.DataFrame(the_bcas)\n",
    "bca_cis[['10', '20', '50', '90']] = bca_cis[['10', '20', '50', '90']].astype('int')\n",
    "bca_cis['b-method'] = 'BCa'\n",
    "bca_d = bca_cis.copy()\n",
    "bca_d.rename(columns={\"b-method\":\"Methode\"}, inplace=True)\n",
    "styled_table_bca = bca_d.style.set_table_styles(table_css_styles)\n",
    "bcas = bca_cis.reset_index()\n",
    "# bcas[['10', '20', '50', '90']] = bcas[['10', '20', '50', '90']].astype('int')\n",
    "# bcas['b-method'] = 'BCa'\n",
    "\n",
    "# bootstrap percentile confidence intervals\n",
    "# resample the survey totals for n times to help\n",
    "# define the range of the sampling statistic\n",
    "# the number of reps\n",
    "n=5000\n",
    "\n",
    "# keep the observed values\n",
    "observed_median = dt_all[unit_label].median()\n",
    "observed_tenth = dt_all[unit_label].quantile(.15)\n",
    "\n",
    "the_cis = {}\n",
    "\n",
    "for a_rank in quantiles:\n",
    "    # for the median\n",
    "    sim_ptile = []\n",
    "\n",
    "    # for the tenth percentile\n",
    "    sim_ten = []\n",
    "    for element in np.arange(n):\n",
    "        less = dt_all[unit_label].sample(n=len(dt_all), replace=True)    \n",
    "        a_ptile = less.quantile(a_rank)\n",
    "        sim_ptile.append(a_ptile)\n",
    "    # get the upper and lower range of the test statistic disrtribution:\n",
    "    a_min = np.percentile(sim_ptile, 2.5)\n",
    "    a_max = np.percentile(sim_ptile, 97.5)\n",
    "    \n",
    "    # add the observed value and update the dict\n",
    "    the_cis.update({F\"{int(a_rank*100)}\":{'2.5% ci':a_min, \"Beobachtung\": q_vals[a_rank], '97.5% ci': a_max}})\n",
    "\n",
    "# make df\n",
    "p_cis = pd.DataFrame(the_cis)\n",
    "p_cis = p_cis.astype(\"int\")\n",
    "p_cis['b-method'] = '%'\n",
    "p_cd = p_cis.copy()\n",
    "p_cd.rename(columns={\"b-method\":\"Methode\"}, inplace=True)\n",
    "styled_table = p_cd.style.set_table_styles(table_css_styles)\n",
    "p_cis.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(6,4))\n",
    "\n",
    "axone = axs\n",
    "\n",
    "\n",
    "data = p_cis.values\n",
    "sut.hide_spines_ticks_grids(axone)\n",
    "\n",
    "colLabels = [F\"{x}.\" for x in p_cis.columns[:-1]]\n",
    "colLabels.append('Methode')\n",
    "\n",
    "table_one = sut.make_a_table(axone, data, colLabels=colLabels, colWidths=[.25,*[.15]*5], bbox=[0, 0, 1, 1])\n",
    "\n",
    "\n",
    "table_one.set_fontsize(12)\n",
    "\n",
    "                        \n",
    "table_one.get_celld()[(0,0)].get_text().set_text(\"Perzentile\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "figure_name = f\"ci_percentile_bootstrap\"\n",
    "ci_percentile_bootstrap_file_name = f'{save_fig_prefix}{figure_name}.jpeg'\n",
    "save_figure_kwargs.update({\"fname\":ci_percentile_bootstrap_file_name})\n",
    "plt.savefig(**save_figure_kwargs)\n",
    "\n",
    "# figure caption\n",
    "ci_percentile_bootstrap_notes = [\n",
    "    \"Konfidenzintervalle, die durch eine 5.000-fache Wiederholungsstichprobe der \",\n",
    "    \"Umfrageergebnisse für jede Bedingung berechnet wurden\"\n",
    "]\n",
    "\n",
    "ci_percentile_bootstrap_notes = ''.join(ci_percentile_bootstrap_notes)\n",
    "\n",
    "glue(\"ci_percentile_bootstrap_notes\", ci_percentile_bootstrap_notes, display=False)\n",
    "\n",
    "glue(\"ci_percentile_bootstrap\", styled_table, display=False)\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{glue:figure} ci_percentile_bootstrap\n",
    "---\n",
    "name: \"ci_percentile_bootstrap\"\n",
    "---\n",
    "{glue:text}`blank_caption` \n",
    "\n",
    "```\n",
    "{numref}`Abbildung {number}: <ci_percentile_bootstrap>` {glue:text}`ci_percentile_bootstrap_notes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(6,4))\n",
    "\n",
    "axtwo = axs\n",
    "# styled_table = bcas.style.set_table_styles(table_css_styles)\n",
    "# data = p_cis.values\n",
    "sut.hide_spines_ticks_grids(axtwo)\n",
    "colLabels = [F\"{x}.\" for x in p_cis.columns[:-1]]\n",
    "colLabels.append('Methode')\n",
    "\n",
    "table_two = sut.make_a_table(axtwo, bcas.values, colLabels=colLabels, colWidths=[.25,*[.15]*5], bbox=[0, 0, 1, 1])\n",
    "\n",
    "table_two.set_fontsize(12)    \n",
    "                        \n",
    "table_two.get_celld()[(0,0)].get_text().set_text(\"BCa\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "figure_name = f\"bca_bootstrap\"\n",
    "bca_bootstrap_file_name = f'{save_fig_prefix}{figure_name}.jpeg'\n",
    "save_figure_kwargs.update({\"fname\":bca_bootstrap_file_name})\n",
    "plt.savefig(**save_figure_kwargs)\n",
    "\n",
    "# figure caption\n",
    "bca_bootstrap_notes = [\n",
    "    \"Die gleichen Intervalle unter Verwendung der verzerrungskorrigierten Methode.\"\n",
    "]\n",
    "\n",
    "bca_bootstrap_notes = ''.join(bca_bootstrap_notes)\n",
    "\n",
    "glue(\"bca_bootstrap_notes\", bca_bootstrap_notes, display=False)\n",
    "\n",
    "\n",
    "glue(\"bca_bootstrap\", styled_table_bca, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{glue:figure} bca_bootstrap\n",
    ":name: \"bca_bootstrap\"\n",
    "\n",
    "\n",
    "{glue:text}`blank_caption` \n",
    "\n",
    "```\n",
    "\n",
    "{numref}`Abbildung {number}: <bca_bootstrap>` {glue:text}`bca_bootstrap_notes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{figure} resources/images/baselines/mullermatte_bielersee31_01_2021.png\n",
    ":name: mullermatte_bielersee31_01_2021\n",
    "\n",
    "\n",
    "{glue:text}`blank_caption` \n",
    "\n",
    "```\n",
    "\n",
    "{numref}`Abbildung {number}: <mullermatte_bielersee31_01_2021>` Beispiel für das Konfidenzintervall: Das Ergebnis der Datenerhebungen in Biel am 31.01.2021 war grösser als der Medianwert für alle Datenerhebungen. Es wurden 123 Objekte (p) über 40 Meter (m) Uferlinie gesammelt. Zunächst wird der Wert der Datenerhebungen in Abfallobjekte pro Meter (p/m) umgerechnet und dann mit der erforderlichen Anzahl von Metern (100) multipliziert: $(pcs/m)*100 = (123_{pcs} / 40_m)*100 m \\approxeq \\text{313 p/100 m}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "subsection_kis_two = Paragraph(\"Vergleich der Bootstrap-KIs\", style=subsection_title)\n",
    "\n",
    "fig_9_cap = Paragraph(bca_bootstrap_notes, style=caption_style)\n",
    "\n",
    "figure_kwargs.update({\n",
    "    \"image_file\":bca_bootstrap_file_name,\n",
    "    \"original_width\":15.24,\n",
    "    \"original_height\":10.16,\n",
    "    \"desired_width\": 7.9,\n",
    "    \"caption\": fig_9_cap,\n",
    "    \"caption_height\": 1.5,\n",
    "})\n",
    "\n",
    "figure_nine = figureAndCaptionTable(**figure_kwargs)\n",
    "\n",
    "fig_10_cap = Paragraph(ci_percentile_bootstrap_notes, style=caption_style)\n",
    "figure_kwargs.update({\n",
    "    \"image_file\":ci_percentile_bootstrap_file_name,\n",
    "    \"original_width\":15.24,\n",
    "    \"original_height\":10.16,\n",
    "    \"desired_width\": 7.9,\n",
    "    \"caption\": fig_10_cap,\n",
    "})\n",
    "\n",
    "figure_ten = figureAndCaptionTable(**figure_kwargs)\n",
    "\n",
    "table_data = [[figure_nine, figure_ten]]\n",
    "\n",
    "fig_nine_ten = Table(table_data, colWidths=[8.5*cm, 8.5*cm])                      \n",
    "\n",
    "new_components = [\n",
    "    smallest_space,\n",
    "    subsection_kis_two,\n",
    "    small_space,\n",
    "    fig_nine_ten,\n",
    "   \n",
    "]\n",
    "\n",
    "pdfcomponents = addToDoc(new_components, pdfcomponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Basiswerte \n",
    "\n",
    "Für diesen Datensatz sind die Unterschiede zwischen den mit der Bca-Methode oder der Perzentilmethode berechneten KIs minimal. Die Bca-Methode wird verwendet, um die Basiswerte und KIs anzugeben.\n",
    "\n",
    "#### Basismedianwert aller Erhebungsergebnisse \n",
    "\n",
    "Wenn man nur Datenerhebungen mit einer Länge von mehr als 10 Metern berücksichtigt und Objekte mit einer Grösse von weniger als 2,5 cm ausschliesst, __lag der Medianwert aller Daten bei 181 p/100 m mit einem KI von 147 p/100 m – 213 p/100 m. Der gemeldete Medianwert für die EU lag bei 133 p/100 m__ und damit im Bereich des KI der IQAASL-Erhebungsergebnisse. Während der Medianwert in der Schweiz höher ist, liegt der Mittelwert der EU-Studie bei 504 p/100 m gegenüber 341 p/100 m in der Schweiz.{cite}`eubaselines`\n",
    "\n",
    "Das deutet darauf hin, dass die höheren Extremwerte in der Meeresumwelt wahrscheinlicher waren, aber der erwartete Medianwert beider Datensätze ist ähnlich. \n",
    "\n",
    "#### Median-Basislinie und KI pro Erhebungsgebiet \n",
    "\n",
    "In der vorliegenden Studie wurden in drei von vier Erhebungsgebieten mehr als 40 Erhebungen durchgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "bassins = [\"linth\", \"aare\", \"rhone\"]\n",
    "the_sas = {}\n",
    "for a_bassin in bassins:\n",
    "    an_int = int(a_rank*100)\n",
    "    a_result = compute_bca_ci(dt_all[dt_all.river_bassin == a_bassin][unit_label].to_numpy(), .05, n_reps=5000, statfunction=np.percentile, stat_param=50)\n",
    "    observed = np.percentile(dt_all[dt_all.river_bassin == a_bassin][unit_label].to_numpy(), 50)\n",
    "    the_sas.update({a_bassin:{'2.5% ci':a_result[0], \"Beobachtung\": observed, '97.5% ci': a_result[1]}})\n",
    "\n",
    "sas = pd.DataFrame(the_sas)\n",
    "\n",
    "new_colnames = {\n",
    "    \"linth\":\"Linth\",\n",
    "    \"rhone\": \"Rhône\",\n",
    "    \"aare\":\"Aare\",\n",
    "    \"b-method\": \"Methode\"\n",
    "}\n",
    "sas['b-method'] = 'bca'\n",
    "sas.rename(columns=new_colnames, inplace=True)\n",
    "\n",
    "\n",
    "sas[\"Linth\"] = sas[\"Linth\"].apply(lambda x: featuredata.thousandsSeparator(int(x)))\n",
    "sas[\"Aare\"] = sas[\"Aare\"].apply(lambda x: featuredata.thousandsSeparator(int(x)))\n",
    "sas[\"Rhône\"] = sas[\"Rhône\"].apply(lambda x: featuredata.thousandsSeparator(int(x)))\n",
    "sas_d = sas.copy()  \n",
    "styled_table = sas.style.set_table_styles(table_css_styles)\n",
    "sas = sas.reset_index()\n",
    "fig, axs = plt.subplots(figsize=(7,4))\n",
    "\n",
    "data = sas.values\n",
    "sut.hide_spines_ticks_grids(axs)\n",
    "\n",
    "table_one = sut.make_a_table(axs, data, colLabels=sas.columns, colWidths=[.25,*[.15]*5], bbox=[0, 0, 1, 1])\n",
    "table_one.get_celld()[(0,0)].get_text().set_text(\" \")\n",
    "\n",
    "figure_name = \"ci_survey_area\"\n",
    "ci_survey_area_file_name = f'{save_fig_prefix}{figure_name}.jpeg'\n",
    "save_figure_kwargs.update({\"fname\":ci_survey_area_file_name})\n",
    "plt.savefig(**save_figure_kwargs)\n",
    "\n",
    "# figure caption\n",
    "ci_survey_area_notes = [\n",
    "    \"Der Median und das 95-Prozent-Konfidenzintervall der Erhebungsgebiete Linth, Aare \",\n",
    "    \"und Rhône. Das Erhebungsgebiet Tessin ist mangels ausreichender Anzahl von Datenerhebungen \",\n",
    "    \"nicht enthalten.\"\n",
    "]\n",
    "\n",
    "ci_survey_area_notes = ''.join(ci_survey_area_notes)\n",
    "\n",
    "glue(\"ci_survey_area_notes\", ci_survey_area_notes, display=False)\n",
    "\n",
    "glue(\"ci_survey_area\", styled_table, display=False)\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{glue:figure} ci_survey_area\n",
    "---\n",
    "name: 'ci_survey_area'\n",
    "---\n",
    "{glue:text}`blank_caption` \n",
    "```\n",
    "{numref}`Abbildung %s: <ci_survey_area>` {glue:text}`ci_survey_area_notes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig_11_caption = [\n",
    "    \"<b>Rechts:</b> Beispiel für das Konfidenzintervall: Das Ergebnis der Datenerhebungen in Biel am 31.01.2021 war grösser als der Medianwert \",\n",
    "    \"für alle Datenerhebungen. Es wurden 123 Objekte (p) über 40 Meter (m) Uferlinie gesammelt. Zunächst wird der Wert der \",\n",
    "    \"Datenerhebungen in Abfallobjekte pro Meter (p/m) umgerechnet und dann mit der erforderlichen Anzahl von Metern (100) multipliziert: \",\n",
    "    \"(p/m)*100 = (123_{p} / 40_m)*100 m ≈313 p/100 m\"\n",
    "]\n",
    "\n",
    "fig_11_caption = ''.join(fig_11_caption)\n",
    "\n",
    "fig_11_caption = Paragraph(fig_11_caption, style=caption_style)\n",
    "figure_kwargs.update({\n",
    "    \"image_file\":\"resources/images/baselines/mullermatte_bielersee31_01_2021.png\",\n",
    "    \"original_width\":20.8,\n",
    "    \"original_height\":15.4,\n",
    "    \"desired_width\": 7.9,\n",
    "    \"caption\": None,\n",
    "})\n",
    "\n",
    "figure_11 = figureAndCaptionTable(**figure_kwargs)\n",
    "\n",
    "subsection_survey_area_baselines = Paragraph(\"Basiswerte\", subsection_title)\n",
    "\n",
    "p_28 = [\n",
    "    \"Für diesen Datensatz sind die Unterschiede zwischen den mit der \",\n",
    "    \"Bca-Methode oder der Perzentilmethode berechneten KIs minimal. \",\n",
    "    \"Die Bca-Methode wird verwendet, um die Basiswerte und KIs anzugeben.\"\n",
    "]\n",
    "\n",
    "p_28= makeAParagraph(p_28)\n",
    "\n",
    "bold_header_cis = Paragraph(\"Basismedianwert aller Erhebungsergebnisse\", style=bold_block)\n",
    "\n",
    "p_29 = [\n",
    "    \"Wenn man nur Datenerhebungen mit einer Länge von mehr als 10 Metern berücksichtigt \",\n",
    "    \"und Objekte mit einer Grösse von weniger als 2,5 cm ausschliesst, <b>lag der Medianwert \",\n",
    "    \"aller Daten bei 181 p/100 m mit einem KI von 147 p/100 m – 213 p/100 m. Der gemeldete \",\n",
    "    \"Medianwert für die EU lag bei 133 p/100 m</b> und damit im Bereich des KI der \",\n",
    "    \"IQAASL-Erhebungsergebnisse. Während der Medianwert in der Schweiz höher ist, liegt der \",\n",
    "    \"Mittelwert der EU-Studie bei 504 p/100 m gegenüber 341 p/100 m in der Schweiz.\"\n",
    "]\n",
    "\n",
    "p_30 = [\n",
    "    \"Das deutet darauf hin, dass die höheren Extremwerte in der Meeresumwelt wahrscheinlicher \",\n",
    "    \"waren, aber der erwartete Medianwert beider Datensätze ist ähnlich.\"\n",
    "]\n",
    "\n",
    "p_29_to_30 = sectionParagraphs([p_29, p_30], smallspace=smallest_space)\n",
    "\n",
    "bold_header_survey_area_cis = Paragraph(\"Median-Basislinie und KI pro Erhebungsgebiet\", style=bold_block)\n",
    "\n",
    "p_31 = [\n",
    "    \"In der vorliegenden Studie wurden in drei von vier Erhebungsgebieten mehr als 40 Erhebungen durchgeführt.\"\n",
    "]\n",
    "\n",
    "p_31 = makeAParagraph(p_31)\n",
    "\n",
    "table_data = [[[subsection_survey_area_baselines, small_space, p_28, smallest_space, fig_11_caption], figure_11]]\n",
    "\n",
    "baseline_title_figure = Table(table_data, style=featuredata.side_by_side_style_figure_right, colWidths=[8.3*cm, 8.3*cm])\n",
    "\n",
    "fig_12_caption = ''.join(ci_survey_area_notes)\n",
    "\n",
    "fig_12_caption = f'<b>Links:</b> {fig_12_caption}'\n",
    "\n",
    "fig_12_caption = Paragraph(fig_12_caption, style=caption_style)\n",
    "figure_kwargs.update({\n",
    "    \"image_file\":ci_survey_area_file_name,\n",
    "    \"original_width\":17.78,\n",
    "    \"original_height\":10.16,\n",
    "    \"desired_width\": 7.9,\n",
    "    \"caption\": None,\n",
    "})\n",
    "\n",
    "figure_12 = figureAndCaptionTable(**figure_kwargs)\n",
    "\n",
    "table_data =  [[figure_12, [bold_header_survey_area_cis, smallest_space, p_31, smallest_space, fig_12_caption]]]\n",
    "survey_area_cis = Table(table_data, colWidths=[8.3*cm, 8.3*cm], style=featuredata.side_by_side_style_figure_left)         \n",
    "\n",
    "new_components = [\n",
    "    small_space,\n",
    "    baseline_title_figure,\n",
    "    smallest_space,\n",
    "    bold_header_survey_area_cis,\n",
    "    smallest_space,\n",
    "    *p_29_to_30,\n",
    "    survey_area_cis,\n",
    "    \n",
    "]\n",
    "\n",
    "pdfcomponents = addToDoc(new_components, pdfcomponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Extremwerte \n",
    "\n",
    "Wie bereits erwähnt, werden Extremwerte (EVs) oder Ausreisser bei der Berechnung von Basislinien oder CIs nicht aus den Daten ausgeschlossen. Die Identifizierung von EVs und wo und wann sie auftreten, ist jedoch ein wesentlicher Bestandteil des Überwachungsprozesses. \n",
    "\n",
    "Das Auftreten von Extremwerten kann den Durchschnitt der Daten und die Interpretation der Erhebungsergebnisse beeinflussen. Laut dem GFS-Bericht:\n",
    "\n",
    ">  Die Methodik zur Identifizierung von Extremwerten kann entweder auf einem Expertenurteil beruhen oder auf statistischen und modellierenden Ansätzen, wie der Anwendung von Tukey's Box Plots zur Erkennung potenzieller Ausreisser. Für schiefe Verteilungen ist der angepasste Boxplot besser geeignet. {cite}`eubaselines`\n",
    "\n",
    "### Extremwerte definieren\n",
    "\n",
    "Die Referenzen geben keine Hinweise auf den numerischen Wert eines EV. Im Gegensatz zu dem Zielwert von 20 p/100 m oder dem 15. Perzentil bleibt die Definition eines EV der Person überlassen, die die Daten interpretiert. Die Obergrenze von Tukeys Boxplot (unangepasst) ist ungefähr das 90. Perzentil der Daten. Diese Methode ist mit der Bewertungsmetrik kompatibel, und Boxplots lassen sich visuell relativ leicht auflösen.\n",
    "\n",
    "#### Angepasste Boxplots \n",
    "\n",
    "Tukeys Boxplot wird verwendet, um die Verteilung eines univariaten Datensatzes zu visualisieren. Die Proben, die innerhalb des ersten Quartils (25 %) und des dritten Quartils (75 %) liegen, werden als innerhalb des inneren Quartilsbereichs (IQR) betrachtet. Punkte, die ausserhalb des inneren Quartils liegen, werden als Ausreisser betrachtet, wenn ihr Wert grösser oder kleiner als einer der beiden Grenzwerte ist:\n",
    "\n",
    "* Untere Grenze =   $Q_1 - (1.5*IQR)$\n",
    "* Obergrenze =   $Q_3 + (1.5*IQR)$\n",
    "\n",
    "Die Grenzen werden erweitert oder reduziert, um der Form der Daten besser zu entsprechen. Infolgedessen repräsentieren die oberen und unteren Grenzen einen grösseren Wertebereich in Bezug auf das Perzentil-Ranking als bei der unangepassten Version.{cite}`tukeysbox` {cite}`medcouple` \n",
    "\n",
    "Die neue Berechnung sieht wie folgt aus: \n",
    "\n",
    "* Untere Grenze = $Q_1 - (1.5e^{-4MC}*IQR)$\n",
    "\n",
    "* Obergrenze = $Q_3 + (1.5e^{3MC}*IQR)$\n",
    "\n",
    "Die Grenzen werden erweitert oder reduziert, um der Form der Daten besser zu entsprechen. Infolgedessen repräsentieren die oberen und unteren Grenzen einen grösseren Wertebereich in Bezug auf das Perzentil-Ranking als bei der unangepassten Version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "extreme_section = Paragraph(\"Extremewerte\", section_title)\n",
    "p_32 = [\n",
    "    \"Wie bereits erwähnt, werden Extremwerte (EVs) oder Ausreisser bei der Berechnung \",\n",
    "    \"von Basislinien oder CIs nicht aus den Daten ausgeschlossen. Die Identifizierung \",\n",
    "    \"von EVs und wo und wann sie auftreten, ist jedoch ein wesentlicher Bestandteil des \",\n",
    "    \"Überwachungsprozesses.\"\n",
    "]\n",
    "\n",
    "p_33 = [\n",
    "    \"Das Auftreten von Extremwerten kann den Durchschnitt der Daten und die Interpretation \",\n",
    "    \"der Erhebungsergebnisse beeinflussen. Laut dem GFS-Bericht:\"\n",
    "]\n",
    "\n",
    "p_32_to_33 = sectionParagraphs([p_32, p_33], smallspace=smallest_space)\n",
    "\n",
    "p_34 = [\n",
    "    \"Die Methodik zur Identifizierung von Extremwerten kann entweder auf einem \",\n",
    "    \"Expertenurteil beruhen oder auf statistischen und modellierenden Ansätzen, wie\",\n",
    "    \"der Anwendung von Tukey's Box Plots zur Erkennung potenzieller Ausreisser. Für \",\n",
    "    \"schiefe Verteilungen ist der angepasste Boxplot besser geeignet.\",\n",
    "    '<a href=\"#HG19\" color=\"blue\">(HG19)</a>'\n",
    "]\n",
    "\n",
    "p_34 = makeAParagraph(p_34, style=block_quote_style)\n",
    "\n",
    "extreme_def = Paragraph(\"Extremwerte definieren\", style=subsection_title)\n",
    "\n",
    "p_35 = [\n",
    "    \"Die Referenzen geben keine Hinweise auf den numerischen Wert eines EV. Im Gegensatz zu dem \",\n",
    "    \"Zielwert von 20 p/100 m oder dem 15. Perzentil bleibt die Definition eines EV der Person \",\n",
    "    \"überlassen, die die Daten interpretiert. Die Obergrenze von Tukeys Boxplot (unangepasst) \",\n",
    "    \"ist ungefähr das 90. Perzentil der Daten. Diese Methode ist mit der Bewertungsmetrik \",\n",
    "    \"kompatibel, und Boxplots lassen sich visuell relativ leicht auflösen.\"\n",
    "]\n",
    "\n",
    "p_35 = makeAParagraph(p_35)\n",
    "\n",
    "boxplots_def = Paragraph(\"Angepasste Boxplots\", style=subsection_title)\n",
    "\n",
    "\n",
    "p_36 = [\n",
    "    \"Tukeys Boxplot wird verwendet, um die Verteilung eines univariaten Datensatzes \",\n",
    "    \"zu visualisieren. Die Proben, die innerhalb des ersten Quartils (25 %) und des dritten \",\n",
    "    \"Quartils (75 %) liegen, werden als innerhalb des inneren Quartilsbereichs (IQR) betrachtet. \",\n",
    "    \"Punkte, die ausserhalb des inneren Quartils liegen, werden als Ausreisser betrachtet, \",\n",
    "    \"wenn ihr Wert grösser oder kleiner als einer der beiden Grenzwerte ist:\"\n",
    "]\n",
    "\n",
    "p_36 = makeAParagraph(p_36)\n",
    "\n",
    "tukeys_normal = [\n",
    "    \"Untere Grenze = Q_1 - (1.5*IQR)\",\n",
    "    \"Obergrenze = Q_3 + (1.5*IQR)\"\n",
    "]\n",
    "\n",
    "tukeys_normal = makeAList(tukeys_normal)\n",
    "\n",
    "p_37 = [\n",
    "    \"Bei der Anpassung des Boxplots wird die Konstante 1,5 durch einen anderen Parameter \",\n",
    "    \"ersetzt. Dieser Parameter wird mit einer Methode namens Medcouple (MC) berechnet und \",\n",
    "    \"das Ergebnis dieser Methode auf die Konstante 1,5 angewendet.\",\n",
    "    '<a href=\"#HV08\" color=\"blue\">(HV08)</a><a href=\"#SP\" color=\"blue\">(SP)</a>'\n",
    "]\n",
    "\n",
    "p_37 = makeAParagraph(p_37)\n",
    "\n",
    "medcouple_header = Paragraph(\"Die neue Berechnung sieht wie folgt aus:\", style=featuredata.p_style)\n",
    "\n",
    "medcouple_list = [\n",
    "    \"Untere Grenze = Q_1 - (1.5e^{-4MC}*IQR)\",\n",
    "    \"Obergrenze = Q_3 + (1.5e^{3MC}*IQR)\",\n",
    "]\n",
    "\n",
    "medcouple_list = makeAList(medcouple_list)\n",
    "\n",
    "p_38 = [\n",
    "    \"Die Grenzen werden erweitert oder reduziert, um der Form der Daten besser zu entsprechen. \",\n",
    "    \"Infolgedessen repräsentieren die oberen und unteren Grenzen einen grösseren Wertebereich \",\n",
    "    \"in Bezug auf das Perzentil-Ranking als bei der unangepassten Version.\"\n",
    "]\n",
    "\n",
    "p_38 = makeAParagraph(p_38)\n",
    "\n",
    "spbib = [\n",
    "    \"Statsmodels: econometric and statistical modeling with python. URL: \",\n",
    "    \"https://www.statsmodels.org/stable/generated/statsmodels.stats.stattools.medcouple.html.\"\n",
    "]\n",
    "spbib = ''.join(spbib)\n",
    "\n",
    "spmodel = makeAParagraph(featuredata.makeBibEntry(name=\"SP\", team=\"Skipper Seabold and Josef Perktold\", pub=spbib))\n",
    "\n",
    "hvo8pub = [\n",
    "    \"An adjusted boxplot for skewed distributions. Computational Statistics & Data Analysis, \",\n",
    "    \"52:5186–5201, 08 2008. doi:10.1016/j.csda.2007.11.008.\"\n",
    "]\n",
    "hvo8pub = ''.join(hvo8pub)\n",
    "\n",
    "hv08 = makeAParagraph(featuredata.makeBibEntry(name=\"HV08\", team=\"Mia Hubert and Ellen Vandervieren\", pub=hvo8pub))\n",
    "\n",
    "references = [*references, smallest_space,spmodel, smallest_space, hv08]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# implementation of medcouple\n",
    "a_whis = medcouple(dt_all[unit_label].to_numpy())\n",
    "\n",
    "# get the ecdf \n",
    "ecdf = ECDF(dt_all[unit_label].to_numpy())\n",
    "\n",
    "# quantiles and IQR of the data\n",
    "q1 = dt_all[unit_label].quantile(0.25)\n",
    "q3 =dt_all[unit_label].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "# the upper and lower limit of extreme values unadjusted:\n",
    "limit_lower = q1 - 1.5*iqr\n",
    "limit_upper = q3 + 1.5*iqr\n",
    "\n",
    "# the upper and lower limit of extreme values adjusted:\n",
    "a_fence = q1 - (1.5*(math.exp((-4*a_whis))))*iqr\n",
    "a_2fence = q3 + (1.5*(math.exp((3*a_whis))))*iqr\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,6))\n",
    "box_props = {\n",
    "    'boxprops':{'facecolor':'none', 'edgecolor':'magenta'},\n",
    "    'medianprops':{'color':'magenta'},\n",
    "    'whiskerprops':{'color':'magenta'},\n",
    "    'capprops':{'color':'magenta'}\n",
    "}\n",
    "\n",
    "sns.stripplot(data=dt_all, y=unit_label, ax=ax, zorder=1, color='black', jitter=.35, alpha=0.3, s=8)\n",
    "sns.boxplot(data=dt_all, y=unit_label, ax=ax, zorder=3, orient='v', showfliers=False, **box_props)\n",
    "ax.axhline(y=a_2fence, xmin=.25, xmax=.75, c='magenta', zorder=3,)\n",
    "ax.set_ylabel(unit_label, **ck.xlab_k14)\n",
    "\n",
    "ax.tick_params(which='both', axis='both', labelsize=14)\n",
    "ax.tick_params(which='both', axis='x', bottom=False)\n",
    "\n",
    "ax.set_ylim(0, a_2fence+200)\n",
    "\n",
    "ax.annotate(\"bereinigt\",\n",
    "                  xy=(0, a_2fence), xycoords='data',\n",
    "                  xytext=(.2,a_2fence-200), textcoords='data',\n",
    "                  size=14, va=\"center\", ha=\"center\",\n",
    "                  bbox=dict(boxstyle=\"round4\", fc=\"w\", alpha=0.5),\n",
    "                  arrowprops=dict(arrowstyle=\"-|>\",\n",
    "                                  connectionstyle=\"arc3,rad=-0.2\",\n",
    "                                  fc=\"black\"),)\n",
    "\n",
    "ax.annotate(\"nicht bereinigt\",\n",
    "                  xy=(0, limit_upper), xycoords='data',\n",
    "                  xytext=(-.2,limit_upper+400), textcoords='data',\n",
    "                  size=14, va=\"center\", ha=\"center\",\n",
    "                  bbox=dict(boxstyle=\"round4\", fc=\"w\",alpha=0.5),\n",
    "                  arrowprops=dict(arrowstyle=\"-|>\",\n",
    "                                  connectionstyle=\"arc3,rad=-0.2\",\n",
    "                                  fc=\"black\"),)\n",
    "ax.grid(visible=True, which='major', axis='y', linestyle='-', linewidth=1, c='black', alpha=.1, zorder=0)\n",
    "\n",
    "figure_name = \"tukeys_example\"\n",
    "tukeys_example_file_name = f'{save_fig_prefix}{figure_name}.jpeg'\n",
    "save_figure_kwargs.update({\"fname\":tukeys_example_file_name})\n",
    "plt.savefig(**save_figure_kwargs)\n",
    "\n",
    "# figure caption\n",
    "tukeys_caption = [\n",
    "    \"Die Grenze, ab der eine Datenerhebung als extrem gilt, erstreckt sich auf das 98. \",\n",
    "    \"Perzentil, wenn die Boxplots angepasst werden, im Gegensatz zum 90. Perzentil, \",\n",
    "    \"wenn die Konstante bei 1,5 belassen wird. \",\n",
    "    \"Der Unterschied zwischen bereinigten und normalen Boxplots. Bereinigt = 1’507 p/100, m\",\n",
    "    \"nicht bereinigt = 755 p/100 m.\"\n",
    "]\n",
    "\n",
    "tukeys_example_notes = ''.join(tukeys_caption)\n",
    "\n",
    "glue(\"tukeys_example_notes\", tukeys_example_notes, display=False)\n",
    "\n",
    "\n",
    "glue(\"tukeys_example\", fig, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{glue:figure} tukeys_example\n",
    "---\n",
    "name: 'tukeys_example'\n",
    "---\n",
    "{glue:text}`blank_caption` \n",
    "```\n",
    "{numref}`Abbildung %s: <tukeys_example>` {glue:text}`tukeys_example_notes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig_13_cap = makeAParagraph(f\"<b>Rechts:</b> {tukeys_caption}\", style=caption_style)\n",
    "\n",
    "figure_kwargs.update({\n",
    "    \"image_file\":tukeys_example_file_name,\n",
    "    \"original_width\":10.16,\n",
    "    \"original_height\":15.24,\n",
    "    \"desired_width\": 7.9,\n",
    "    \"caption\":None,\n",
    "})\n",
    "\n",
    "figure_13 = figureAndCaptionTable(**figure_kwargs)\n",
    "\n",
    "p_40 = [\n",
    "    \"Bei Verwendung der bereinigten Boxplots steigt die Extremwertschwelle (EVT) auf \",\n",
    "    \"über 1600 p/100 m. Die unbereinigten Boxplots liegen jedoch innerhalb des KI des \",\n",
    "    \"erwarteten Perzentils der Erhebungsdaten.\"\n",
    "]\n",
    "\n",
    "p_40 = makeAParagraph(p_40)\n",
    "\n",
    "p_41 = [\n",
    "    \"<b>Unten:</b> Beispiel für bereinigte Extremwerte: St. Gingolph, 12.08.2020. Es \",\n",
    "    \"wurden 514 Objekte (p) über 31 Meter (m) Uferlinie gesammelt. Zuerst wird der Wert \",\n",
    "    \"der Datenerhebungen in Abfallobjekte pro Meter (p/m) umgerechnet und dann mit der \",\n",
    "    \"erforderlichen Anzahl von Metern (100) multipliziert: \",\n",
    "    \"<br/>(p/m)*100 = (514_{p} / 31_m)*100m ≈1652 p/100 m\"\n",
    "]\n",
    "\n",
    "p_41 = makeAParagraph(p_41, style=caption_style)\n",
    "\n",
    "table_data = [[[fig_13_cap, small_space, smallest_space, p_40, small_space, small_space, p_41], figure_13]]\n",
    "\n",
    "tukeys_figure = Table(table_data, style=featuredata.side_by_side_style_figure_right, colWidths=[8.3*cm, 8.3*cm])\n",
    "    \n",
    "new_components = [\n",
    "    small_space,\n",
    "    extreme_section,\n",
    "    small_space,\n",
    "    *p_32_to_33,\n",
    "    p_34,\n",
    "    smallest_space,\n",
    "    extreme_def,\n",
    "    smallest_space,\n",
    "    p_35,\n",
    "    smallest_space,\n",
    "    boxplots_def,\n",
    "    smallest_space,\n",
    "    p_36,\n",
    "    smallest_space,\n",
    "    tukeys_normal,\n",
    "    smallest_space,\n",
    "    p_37,\n",
    "    smallest_space,\n",
    "    medcouple_header,\n",
    "    smallest_space,\n",
    "    medcouple_list,\n",
    "    smallest_space,\n",
    "    p_38,\n",
    "    smallest_space,\n",
    "    tukeys_figure,  \n",
    "]\n",
    "\n",
    "pdfcomponents = addToDoc(new_components, pdfcomponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Der Unterschied zwischen bereinigten und normalen Boxplots. Bereinigt = 1507 p/100 m, nicht bereinigt = 755 p/100 m.\n",
    "\n",
    "Bei Verwendung der bereinigten Boxplots steigt die Extremwertschwelle (EVT) auf über 1600 p/100 m. Die unbereinigten Boxplots liegen jedoch innerhalb des KI des erwarteten Perzentils der Erhebungsdaten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{figure} resources/images/baselines/onethous60053pcs100m.jpg\n",
    ":name: onethous60053pcs100m\n",
    "\n",
    "\n",
    "{glue:text}`blank_caption` \n",
    "\n",
    "```\n",
    "\n",
    "{numref}`Abbildung {number}: <onethous60053pcs100m>` Beispiel für bereinigte Extremwerte: St. Gingolph, 12.08.2020. Es wurden 514 Objekte (p) über 31 Meter (m) Uferlinie gesammelt. Zuerst wird der Wert der Datenerhebungen in Abfallobjekte pro Meter (p/m) umgerechnet und dann mit der erforderlichen Anzahl von Metern (100) multipliziert: $(pcs/m)*100 = (514_{pcs} / 31_m)*100m \\approxeq  \\text{1'652 p/100 m}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "figure_kwargs.update({\n",
    "    \"image_file\":\"resources/images/baselines/onethous60053pcs100m.jpg\",\n",
    "    \"original_width\":14.9,\n",
    "    \"original_height\":6.7,\n",
    "    \"desired_width\": 15,\n",
    "    \"caption\":None,\n",
    "})\n",
    "\n",
    "figure_13 = figureAndCaptionTable(**figure_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Modellierung \n",
    "\n",
    "Extremwerte können identifiziert werden, indem man davon ausgeht, dass die Daten zu einer zugrundeliegenden bekannten statistischen Verteilung gehören. Im Allgemeinen wird davon ausgegangen, dass Zähldaten eine Poisson-Verteilung oder eine sehr ähnliche Form aufweisen. Bei der Poisson-Verteilung wird angenommen, dass der Mittelwert gleich der Varianz ist. Die vorliegenden Daten und Strandabfalldaten im Allgemeinen weisen eine hohe Varianz auf, die in der Regel grösser als der Mittelwert ist. \n",
    "\n",
    "Für die negative Binomialverteilung (NB) gilt diese Anforderung nicht. Die NB ist eine Poisson-Verteilung mit dem Parameter λ, wobei λ selbst nicht fest ist, sondern eine Zufallsvariable, die einer Gamma-Verteilung folgt. {cite}`cameron` {cite}`wolfram` {cite}`nbinom` \n",
    "\n",
    "> Der Modellierungsansatz zur Identifizierung von Extremwerten erfolgt dann durch Anpassung der NB-Verteilung an die Daten mittels maximaler Wahrscheinlichkeit (MLE) und Kennzeichnung aller Werte im rechten Schwanz als potenziell extreme Werte, wenn die Wahrscheinlichkeit, dass sie zur angepassten NB-Verteilung gehören, kleiner als z.B. 0,001 ist.  {cite}`threshholdeu`\n",
    "\n",
    "Der MLE ist eine der beiden empfohlenen Methoden zur Modellierung oder Anpassung von Datenwerten an eine angenommene Verteilung:\n",
    "\n",
    "* Methode der Momente (MOM) \n",
    "* MLE: Maximum-Likelihood-Schätzung \n",
    "\n",
    "##### Methode der Momente (MOM)\n",
    "\n",
    "Die Methode der Momente geht davon aus, dass die aus der Stichprobe abgeleiteten Parameter den Populationsparametern nahekommen oder gleich sind. Im Falle von Strand-Abfallerhebungen bedeutet dies, dass der Median, der Mittelwert und die Varianz der Stichprobe als gute Annäherung an die tatsächlichen Werte angesehen werden können, wenn alle Strände an allen Seen und Fliessgewässern untersucht werden. \n",
    "\n",
    "Konkret werden die Parameter eines wahrscheinlichen Verteilungsmodells geschätzt, indem sie aus den Beispieldaten berechnet werden. Diese Methode ist einfach anzuwenden, da die meisten Parameterberechnungen für die gängigsten Verteilungen gut bekannt sind. {cite}`srikanta` {cite}`2020SciPy` {cite}`examplemmoments`\n",
    "\n",
    "##### Maximum-Likelihood-Schätzung (MLE)\n",
    "\n",
    "MLE ist eine Methode zur Schätzung der Parameter eines statistischen Modells bei gegebenen Daten. In dieser Hinsicht unterscheidet sie sich nicht von der MOM. Der Unterschied besteht darin, dass bei der MOM die Modellparameter aus den Daten berechnet werden, während bei der MLE die Parameter so gewählt werden, dass die Daten angesichts des statistischen Modells am wahrscheinlichsten sind. \n",
    "\n",
    "Diese Methode ist rechenintensiver als die MOM, hat aber einige Vorteile:\n",
    "\n",
    "* Wenn das Modell korrekt angenommen wird, ist MLE die effizienteste Schätzung.\n",
    "* MLE führt zu unverzerrten Schätzungen in grösseren Stichproben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "model_section_title = Paragraph(\"Modellierung\", section_title)\n",
    "\n",
    "p_42 = [\n",
    "    \"Extremwerte können identifiziert werden, indem man davon ausgeht, dass die Daten zu einer \",\n",
    "    \"zugrundeliegenden bekannten statistischen Verteilung gehören. Im Allgemeinen wird davon ausgegangen, \",\n",
    "    \"dass Zähldaten eine Poisson-Verteilung oder eine sehr ähnliche Form aufweisen. Bei der \",\n",
    "    \"Poisson-Verteilung wird angenommen, dass der Mittelwert gleich der Varianz ist. Die vorliegenden \",\n",
    "    \"Daten und Strandabfalldaten im Allgemeinen weisen eine hohe Varianz auf, die in der Regel grösser \",\n",
    "    \"als der Mittelwert ist.\"\n",
    "]\n",
    "\n",
    "p_43 = [\n",
    "    \"Für die negative Binomialverteilung (NB) gilt diese Anforderung nicht. Die NB ist eine \",\n",
    "    \"Poisson-Verteilung mit dem Parameter λ, wobei λ selbst nicht fest ist, sondern eine \",\n",
    "    \"Zufallsvariable, die einer Gamma-Verteilung folgt.\",\n",
    "    '<a href=\"#CT99\" color=\"blue\">(CT99)</a><a href=\"#Wei20\" color=\"blue\">(Wei20)</a><a href=\"#nbi\" color=\"blue\">(nbi)</a>'\n",
    "]\n",
    "\n",
    "p42_43 = sectionParagraphs([p_42, p_43], smallspace=smallest_space)\n",
    "\n",
    "p_44 = [\n",
    "    '\"Der Modellierungsansatz zur Identifizierung von Extremwerten erfolgt dann durch Anpassung der ',\n",
    "    \"NB-Verteilung an die Daten mittels maximaler Wahrscheinlichkeit (MLE) und Kennzeichnung aller \",\n",
    "    \"Werte im rechten Schwanz als potenziell extreme Werte, wenn die Wahrscheinlichkeit, dass sie \",\n",
    "    'zur angepassten NB-Verteilung gehören, kleiner als z.B. 0,001 ist.\"',\n",
    "    '<a href=\"#VLW20\" color=\"blue\">(VLW20)</a>'\n",
    "]\n",
    "\n",
    "p_44 = makeAParagraph(p_44, style=block_quote_style)\n",
    "\n",
    "p_45 = [\n",
    "    \"Der MLE ist eine der beiden empfohlenen Methoden zur \",\n",
    "    \"Modellierung oder Anpassung von Datenwerten an eine angenommene Verteilung:\"\n",
    "]\n",
    "\n",
    "p_45 = makeAParagraph(p_45)\n",
    "\n",
    "t_models_list =[\n",
    "    \"Methode der Momente (MOM)\",\n",
    "    \"MLE: Maximum-Likelihood-Schätzung\",\n",
    "]\n",
    "\n",
    "t_models_list = makeAList(t_models_list)\n",
    "\n",
    "mom_title = Paragraph(\"Methode der Momente (MOM)\", style=bold_block)\n",
    "\n",
    "p_46 = [\n",
    "    \"Die Methode der Momente geht davon aus, dass die aus der Stichprobe abgeleiteten Parameter \",\n",
    "    \"den Populationsparametern nahekommen oder gleich sind. Im Falle von Strand-Abfallerhebungen \",\n",
    "    \"bedeutet dies, dass der Median, der Mittelwert und die Varianz der Stichprobe als gute Annäherung \",\n",
    "    \"an die tatsächlichen Werte angesehen werden können, wenn alle Strände an allen Seen und Fliessgewässern \",\n",
    "    \"untersucht werden.\"\n",
    "]\n",
    "\n",
    "p_47 = [\n",
    "    \"Konkret werden die Parameter eines wahrscheinlichen Verteilungsmodells geschätzt, \",\n",
    "    \"indem sie aus den Beispieldaten berechnet werden. Diese Methode ist einfach anzuwenden, \",\n",
    "    \"da die meisten Parameterberechnungen für die gängigsten Verteilungen gut bekannt sind.\",\n",
    "    '<a href=\"#MDG18\" color=\"blue\">(MDG18)</a><a href=\"#VGO+20\" color=\"blue\">(VGO+20)</a>'\n",
    "]\n",
    "\n",
    "p_46_47 = sectionParagraphs([p_46, p_47], smallspace=smallest_space)\n",
    "\n",
    "mle_title = Paragraph(\"Maximum-Likelihood-Schätzung (MLE)\", style=bold_block)\n",
    "\n",
    "p_48 = [\n",
    "    \"MLE ist eine Methode zur Schätzung der Parameter eines statistischen Modells bei gegebenen Daten. \",\n",
    "    \"In dieser Hinsicht unterscheidet sie sich nicht von der MOM. Der Unterschied besteht darin, dass \",\n",
    "    \"bei der MOM die Modellparameter aus den Daten berechnet werden, während bei der MLE die Parameter \",\n",
    "    \"so gewählt werden, dass die Daten angesichts des statistischen Modells am wahrscheinlichsten sind.\"\n",
    "]\n",
    "\n",
    "p_49 = [\n",
    "    \"Diese Methode ist rechenintensiver als die MOM, hat aber einige Vorteile:\"\n",
    "]\n",
    "\n",
    "p_48_49 = sectionParagraphs([p_48, p_49], smallspace=smallest_space)\n",
    "\n",
    "mle_list = [\n",
    "    \"Wenn das Modell korrekt angenommen wird, ist MLE die effizienteste Schätzung.\",\n",
    "    \"MLE führt zu unverzerrten Schätzungen in grösseren Stichproben.\"\n",
    "]\n",
    "\n",
    "mle_list = makeAList(mle_list)\n",
    "\n",
    "nbipub = [\n",
    "    \"Scipy stats nbinom. URL: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.nbinom.html\"\n",
    "]\n",
    "\n",
    "nbi = makeAParagraph(featuredata.makeBibEntry(name=\"nbi\", team=\"SciPy, NumFocus\", pub=nbipub[0]))\n",
    "\n",
    "weipub = [\n",
    "    \"negative binomial distribution. from mathworld–a wolfram web resource. 2020. URL: https://mathworld.wolfram.com/NegativeBinomialDistribution.html.\"\n",
    "]\n",
    "\n",
    "wei20 = makeAParagraph(featuredata.makeBibEntry(name=\"Wei20\", team=\"Wolfram-mathworld\", pub=weipub[0]))\n",
    "\n",
    "ct99pub = [\n",
    "    \"Regression analysis of count data. 2nd ed, pages 200. Volume 41. cambridge, 09 1999. doi:10.1017/CBO9780511814365.\"\n",
    "]\n",
    "\n",
    "ct99 = makeAParagraph(featuredata.makeBibEntry(name=\"CT99\", team=\"A. Cameron and Pravin Trivedi\", pub=ct99pub[0]))\n",
    "\n",
    "mdg18pub = [\n",
    "    \"Chapter 3 - distributions and models thereof. In Srikanta Mishra \",\n",
    "    \"and Akhil Datta-Gupta, editors, Applied Statistical Modeling and Data Analytics, pages 31–67. Elsevier, 2018. \",\n",
    "    \"URL: https://www.sciencedirect.com/science/article/pii/B9780128032794000031, doi:https://doi.org/10.1016/B978-0-12-803279-4.00003-1.\"\n",
    "]\n",
    "mdg18pub = ''.join(mdg18pub)\n",
    "\n",
    "mdg18 = makeAParagraph(featuredata.makeBibEntry(name=\"MDG18\", team=\"Srikanta Mishra and Akhil Datta-Gupta.\", pub=mdg18pub))\n",
    "\n",
    "vgo20pub = [\n",
    "    \"SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261–272, 2020. doi:10.1038/s41592-019-0686-2.\"\n",
    "]\n",
    "\n",
    "vgo20team = [\n",
    "    \"Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, \",\n",
    "    \"Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, İlhan Polat, Yu Feng, \",\n",
    "    \"Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, \",\n",
    "    \"Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors.\"\n",
    "]\n",
    "\n",
    "vgo20 = makeAParagraph(featuredata.makeBibEntry(name=\"VGO+20\", team=''.join(vgo20team), pub=vgo20pub[0]))\n",
    "\n",
    "sopub = [\n",
    "    \"Fitting for discrete data: negative binomial, poisson, geometric distribution. \",\n",
    "    \n",
    "]\n",
    "\n",
    "somle = makeAParagraph(featuredata.makeBibEntry(name=\"SO\", team=\"Stack-Overflow\", pub=sopub[0]))\n",
    "\n",
    "references = [*references, smallest_space, ct99, smallest_space, wei20, smallest_space, nbi, smallest_space, mdg18, smallest_space, vgo20, smallest_space, somle]\n",
    "\n",
    "new_components = [\n",
    "    small_space,\n",
    "    figure_13,\n",
    "    small_space,\n",
    "    model_section_title,\n",
    "    small_space,\n",
    "    *p42_43,\n",
    "    p_44,\n",
    "    smallest_space,\n",
    "    p_45,\n",
    "    smallest_space,\n",
    "    t_models_list,\n",
    "    smallest_space,\n",
    "    mom_title,\n",
    "    smallest_space,\n",
    "    *p_46_47,\n",
    "    mle_title,\n",
    "    smallest_space,\n",
    "    *p_48_49,\n",
    "    mle_list,\n",
    "    smallest_space,   \n",
    "]\n",
    "pdfcomponents = addToDoc(new_components, pdfcomponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# implementaion of MLE\n",
    "# https://github.com/pnxenopoulos/negative_binomial/blob/master/negative_binomial/core.py\n",
    "\n",
    "def r_derv(r_var, vec):\n",
    "    ''' Function that represents the derivative of the negbinomial likelihood\n",
    "    '''\n",
    "\n",
    "    total_sum = 0\n",
    "    obs_mean = np.mean(vec)  # Save the mean of the data\n",
    "    n_pop = float(len(vec))  # Save the length of the vector, n_pop\n",
    "\n",
    "    for obs in vec:\n",
    "        total_sum += digamma(obs + r_var)\n",
    "\n",
    "    total_sum -= n_pop*digamma(r_var)\n",
    "    total_sum += n_pop*math.log(r_var / (r_var + obs_mean))\n",
    "\n",
    "    return total_sum\n",
    "\n",
    "def p_equa(r_var, vec):\n",
    "    ''' Function that represents the equation for p in the negbin likelihood\n",
    "     '''\n",
    "    data_sum = sum(vec)\n",
    "    n_pop = float(len(vec))\n",
    "    p_var = 1 - (data_sum / (n_pop * r_var + data_sum))\n",
    "    return p_var\n",
    "\n",
    "def neg_bin_fit(vec, init=0.0001):\n",
    "    ''' Function to fit negative binomial to data\n",
    "    vec: the data vector used to fit the negative binomial distribution\n",
    "   \n",
    "    '''\n",
    "    est_r = newton(r_derv, init, args=(vec,))\n",
    "    est_p = p_equa(est_r, vec)\n",
    "    return est_r, est_p\n",
    "\n",
    "# the data to model\n",
    "vals = dt_all[unit_label].to_numpy()\n",
    "\n",
    "# the variance\n",
    "var = np.var(vals)\n",
    "\n",
    "# the average\n",
    "mean = np.mean(vals)\n",
    "\n",
    "# dispersion\n",
    "p = (mean/var)\n",
    "n = (mean**2/(var-mean))\n",
    "\n",
    "# implementation of method of moments\n",
    "r = stats.nbinom.rvs(n,p, size=len(vals))\n",
    "\n",
    "# format data for charting\n",
    "df = pd.DataFrame({unit_label:vals, 'group':\"Beobachtung\"})\n",
    "df = pd.concat([df, pd.DataFrame({unit_label:r, 'group':'MOM'})])\n",
    "\n",
    "scp = df[df.group == 'MOM'][unit_label].to_numpy()\n",
    "obs = df[df.group == \"Beobachtung\"][unit_label].to_numpy()\n",
    "\n",
    "scpsx = [{unit_label:x, 'model':'MOM'} for x in scp]\n",
    "obsx = [{unit_label:x, 'model':\"Beobachtung\"} for x in obs]\n",
    "\n",
    "# ! implementation of MLE\n",
    "estimated_r, estimated_p = neg_bin_fit(obs, init = 0.0001)\n",
    "\n",
    "# ! use the MLE estimators to generate data\n",
    "som_data = stats.nbinom.rvs(estimated_r, estimated_p, size=len(dt_all))\n",
    "som_datax = pd.DataFrame([{unit_label:x, 'model':'MLE'}  for x in som_data])\n",
    "\n",
    "# combined the different results in to one df\n",
    "data = pd.concat([pd.DataFrame(scpsx), pd.DataFrame(obsx), pd.DataFrame(som_datax)])\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "# the 90th\n",
    "ev = data.groupby('model', as_index=False)[unit_label].quantile(.9)\n",
    "xval={'MOM':0, 'Beobachtung':1, 'MLE':2}\n",
    "ev['x'] = ev.model.map(lambda x: xval[x])\n",
    "box_palette = {'MOM':'salmon', 'MLE':'magenta', 'Beobachtung':'dodgerblue'}\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(10,6))\n",
    "\n",
    "ax=axs[0]\n",
    "axone=axs[1]\n",
    "\n",
    "bw=80\n",
    "\n",
    "sns.histplot(data=data, x=unit_label, ax=ax, hue='model', zorder=2, palette=box_palette, binwidth=bw, element='bars', multiple='stack', alpha=0.4)\n",
    "# sns.histplot(data=data, x=unit_label, ax=ax, hue='model')\n",
    "box_props = {\n",
    "    'boxprops':{'facecolor':'none', 'edgecolor':'black'},\n",
    "    'medianprops':{'color':'black'},\n",
    "    'whiskerprops':{'color':'black'},\n",
    "    'capprops':{'color':'black'}\n",
    "}\n",
    "\n",
    "sns.boxplot(data=data, x='model', y=unit_label, ax=axone, zorder=5, palette=box_palette, showfliers=False, dodge=False, **box_props)\n",
    "sns.stripplot(data=data, x='model', y=unit_label, hue='model', zorder=0,palette=box_palette, ax=axone, s=8, alpha=0.2, dodge=False, jitter=0.4)\n",
    "axone.scatter(x=ev.x.values, y=ev[unit_label].values, label=\"90%\", color='black', s=60)\n",
    "axone.set_ylim(0,np.percentile(r, 95))\n",
    "ax.get_legend().remove()\n",
    "ax.set_ylabel(\"# der Erhebungen\", **ck.xlab_k14)\n",
    "axone.set_ylabel(unit_label, **ck.xlab_k14)\n",
    "\n",
    "handles, labels = axone.get_legend_handles_labels()\n",
    "axone.get_legend().remove()\n",
    "h3=handles[:3]\n",
    "hlast= handles[-1:]\n",
    "axone.set_xlabel(\"\")\n",
    "axone.tick_params(which='both', axis='both', labelsize=14)\n",
    "ax.tick_params(which='both', axis='both', labelsize=14)\n",
    "l3 = labels[:3]\n",
    "llast = labels[-1:]\n",
    "\n",
    "ax.grid(visible=True, which='major', axis='y', linestyle='-', linewidth=1, c='black', alpha=.2, zorder=0)\n",
    "axone.grid(visible=True, which='major', axis='y', linestyle='-', linewidth=1, c='black', alpha=.2, zorder=0)\n",
    "\n",
    "fig.legend([*h3, *hlast], [*l3, *llast], bbox_to_anchor=(.48, .96), loc='upper right', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "figure_name = \"models_compare\"\n",
    "models_compare_file_name = f'{save_fig_prefix}{figure_name}.jpeg'\n",
    "save_figure_kwargs.update({\"fname\":models_compare_file_name})\n",
    "plt.savefig(**save_figure_kwargs)\n",
    "\n",
    "# figure caption\n",
    "models_compare_caption = [\n",
    "    \"<b>Anpassen der Daten an die zugrundeliegende NB-Verteilung.</b> Die beobachteten Erhebungsergebnisse werden \",\n",
    "    \"mit den geschätzten Datenerhebungen unter Verwendung der Methode der Momente und der Maximum-Likelihood-Schätzung \",\n",
    "    \"verglichen. <b>Links:</b> Histogramm der Ergebnisse im Vergleich zu den beobachteten Daten. <b>Rechts:</b> Verteilung \",\n",
    "    \"der Ergebnisse im Vergleich zu den beobachteten Daten mit 90. Perzentil. 90% p/100m:  MLE=825,  Observed=727,  MOM=888\"\n",
    "]\n",
    "\n",
    "models_compare_notes = ''.join(models_compare_caption)\n",
    "\n",
    "glue(\"models_compare\", fig, display=False)\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{glue:figure} models_compare\n",
    "---\n",
    "name: 'models_compare'\n",
    "---\n",
    "{glue:text}`blank_caption` \n",
    "```\n",
    "{numref}`Abbildung %s: <models_compare>` __Anpassen der Daten an die zugrundeliegende NB-Verteilung.__ Die beobachteten Erhebungsergebnisse werden mit den geschätzten Datenerhebungen unter Verwendung der Methode der Momente und der Maximum-Likelihood-Schätzung verglichen. __Links:__ Histogramm der Ergebnisse im Vergleich zu den beobachteten Daten. __Rechts:__ Verteilung der Ergebnisse im Vergleich zu den beobachteten Daten mit 90. Perzentil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig_15_cap = makeAParagraph(models_compare_notes, caption_style)\n",
    "\n",
    "figure_kwargs.update({\n",
    "    \"image_file\":models_compare_file_name,\n",
    "    \"original_width\":25.4,\n",
    "    \"original_height\":15.24,\n",
    "    \"desired_width\": 16,\n",
    "    \"caption\":fig_15_cap,\n",
    "    \"caption_height\":2,\n",
    "})\n",
    "\n",
    "figure_15 = figureAndCaptionTable(**figure_kwargs)\n",
    "\n",
    "new_components = [\n",
    "    smallest_space,\n",
    "    figure_15,\n",
    "    PageBreak()\n",
    "]\n",
    "\n",
    "pdfcomponents = addToDoc(new_components, pdfcomponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# the language setting use lower case: en or de\n",
    "# changing the language may require changing the unit label\n",
    "language = \"de\"\n",
    "unit_label = \"p/100 m\"\n",
    "\n",
    "# the standard date format is \"%Y-%m-%d\" if your date column is\n",
    "# not in this format it will not work.\n",
    "# these dates cover the duration of the IQAASL project\n",
    "start_date = \"2020-03-01\"\n",
    "end_date =\"2021-05-31\"\n",
    "start_end = [start_date, end_date]\n",
    "\n",
    "# the fail rate used to calculate the most common codes is\n",
    "# 50% it can be changed:\n",
    "fail_rate = 50\n",
    "\n",
    "\n",
    "# the label for the aggregation of all data in the region\n",
    "top = \"Alle Erhebungsgebiete\"\n",
    "\n",
    "# define the feature level and components\n",
    "# the feature of interest is the all (all) at the river basin (river_bassin) level.\n",
    "# the label for charting is called 'name'\n",
    "this_feature = {'slug':'all', 'name':\"Alle Erhebungsgebiete\", 'level':'all'}\n",
    "\n",
    "# these are the smallest aggregated components\n",
    "# choices are water_name_slug=lake or river, city or location at the scale of a river bassin \n",
    "# water body or lake maybe the most appropriate\n",
    "this_level = 'river_bassin'\n",
    "\n",
    "# !! End note book variables !!\n",
    "## data\n",
    "# Survey location details (GPS, city, land use)\n",
    "# set the index of the beach data to location slug\n",
    "dfBeaches.set_index(\"slug\", inplace=True)\n",
    "\n",
    "# columns that need to be renamed. Setting the language will automatically\n",
    "# change column names, code descriptions and chart annotations\n",
    "columns={\"% to agg\":\"% agg\", \"% to recreation\": \"% recreation\", \"% to woods\":\"% woods\", \"% to buildings\":\"% buildings\", \"p/100m\":\"p/100 m\"}\n",
    "\n",
    "# !key word arguments to construct feature data\n",
    "# !Note the water type allows the selection of river or lakes\n",
    "# if None then the data is aggregated together. This selection\n",
    "# is only valid for survey-area reports or other aggregated data\n",
    "# that may have survey results from both lakes and rivers.\n",
    "fd_kwargs ={\n",
    "    \"filename\": \"resources/checked_sdata_eos_2020_21.csv\",\n",
    "    \"feature_name\": this_feature['slug'], \n",
    "    \"feature_level\": this_feature['level'], \n",
    "    \"these_features\": this_feature['slug'], \n",
    "    \"component\": this_level, \n",
    "    \"columns\": columns, \n",
    "    \"language\": 'de', \n",
    "    \"unit_label\": unit_label, \n",
    "    \"fail_rate\": 50,\n",
    "    \"code_data\":dfCodes,\n",
    "    \"date_range\": start_end,\n",
    "    \"water_type\": None,    \n",
    "}\n",
    "\n",
    "fdx = featuredata.Components(**fd_kwargs)\n",
    "\n",
    "# call the reports and languages\n",
    "fdx.adjustForLanguage()\n",
    "fdx.makeFeatureData()\n",
    "fdx.locationSampleTotals()\n",
    "fdx.makeDailyTotalSummary()\n",
    "fdx.materialSummary()\n",
    "fdx.mostCommon()\n",
    "fdx.codeGroupSummary()\n",
    "\n",
    "# !keyword args to build period data\n",
    "# the period data is all the data that was collected\n",
    "# during the same period from all the other locations\n",
    "# not included in the feature data. For a survey area\n",
    "# or river bassin these_features = feature_parent and \n",
    "# feature_level = parent_level\n",
    "period_kwargs = {\n",
    "    \"period_data\": fdx.period_data,\n",
    "    \"these_features\": this_feature['slug'],\n",
    "    \"feature_level\":this_feature['level'],\n",
    "    \"feature_parent\":this_feature['slug'],\n",
    "    \"parent_level\": this_feature['level'],\n",
    "    \"period_name\": top,\n",
    "    \"unit_label\": unit_label,\n",
    "    \"most_common\": fdx.most_common.index\n",
    "}\n",
    "period_data = featuredata.PeriodResults(**period_kwargs)\n",
    "\n",
    "# this defines the css rules for the note-book table displays\n",
    "header_row = {'selector': 'th:nth-child(1)', 'props': f'background-color: #FFF;text-align:right;'}\n",
    "table_font = {'selector': 'tr', 'props': 'font-size: 12px;'}\n",
    "table_data = {'selector': 'td', 'props': 'padding:6px;'}\n",
    "heat_map_css_styles = [table_font, header_row, table_data]\n",
    "\n",
    "\n",
    "# !this is the feature data!\n",
    "fd = fdx.feature_data\n",
    "\n",
    "# calling componentsMostCommon gets the results for the most common codes\n",
    "# at the component level\n",
    "components = fdx.componentMostCommonPcsM()\n",
    "\n",
    "# map to proper names for features\n",
    "feature_names = featuredata.river_basin_de\n",
    "\n",
    "# pivot that and quash the hierarchal column index that is created when the table is pivoted\n",
    "mc_comp = components[[\"item\", unit_label, this_level]].pivot(columns=this_level, index=\"item\")\n",
    "mc_comp.columns = mc_comp.columns.get_level_values(1)\n",
    "\n",
    "# insert the proper columns names for display\n",
    "proper_column_names = {x : feature_names[x] for x in mc_comp.columns}\n",
    "mc_comp.rename(columns = proper_column_names, inplace=True)\n",
    "\n",
    "# the aggregated total of the feature is taken from the most common objects table\n",
    "mc_feature = fdx.most_common[unit_label]\n",
    "mc_feature = featuredata.changeSeriesIndexLabels(mc_feature, {x:fdx.dMap.loc[x] for x in mc_feature.index})\n",
    "\n",
    "# the aggregated totals of all the period data\n",
    "mc_period = period_data.parentMostCommon(parent=False)\n",
    "mc_period = featuredata.changeSeriesIndexLabels(mc_period, {x:fdx.dMap.loc[x] for x in mc_period.index})\n",
    "\n",
    "# add the feature, bassin_label and period results to the components table\n",
    "mc_comp[\"Alle Erhebungsgebiete\"]= mc_feature\n",
    "\n",
    "# notebook display style\n",
    "aformatter = {x: featuredata.replaceDecimal for x in mc_comp.columns}\n",
    "mcd = mc_comp.style.format(aformatter).set_table_styles(heat_map_css_styles)\n",
    "mcd = mcd.background_gradient(axis=None, vmin=mc_comp.min().min(), vmax=mc_comp.max().max(), cmap=\"YlOrBr\")\n",
    "\n",
    "# remove the index name and column name labels\n",
    "mcd.index.name = None\n",
    "mcd.columns.name = None\n",
    "\n",
    "# rotate the text on the header row\n",
    "# the .applymap_index method in the\n",
    "# df.styler module is used for this\n",
    "mcd = mcd.applymap_index(featuredata.rotateText, axis=1)\n",
    "\n",
    "glue('baselines_most_common_heat_map', mcd, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# define the feature level and components\n",
    "# the feature of interest is the aare (aare) at the river basin (river_bassin) level.\n",
    "# the label for charting is called 'name'\n",
    "this_feature = {'slug':'aare', 'name':\"Erhebungsgebiet Aare\", 'level':'river_bassin'}\n",
    "\n",
    "# the lake is in this survey area\n",
    "this_bassin = \"aare\"\n",
    "# label for survey area\n",
    "bassin_label = \"Erhebungsgebiet Aare\"\n",
    "\n",
    "# these are the smallest aggregated components\n",
    "# choices are water_name_slug=lake or river, city or location at the scale of a river bassin \n",
    "# water body or lake maybe the most appropriate\n",
    "this_level = 'water_name_slug'\n",
    "\n",
    "# identify the lakes of interest for the survey area\n",
    "lakes_of_interest = [\"neuenburgersee\", \"thunersee\", \"bielersee\", \"brienzersee\"]\n",
    "\n",
    "# columns that need to be renamed. Setting the language will automatically\n",
    "# change column names, code descriptions and chart annotations\n",
    "columns={\"% to agg\":\"% agg\", \"% to recreation\": \"% recreation\", \"% to woods\":\"% woods\", \"% to buildings\":\"% buildings\", \"p/100m\":\"p/100 m\"}\n",
    "\n",
    "# !key word arguments to construct feature data\n",
    "# !Note the water type allows the selection of river or lakes\n",
    "# if None then the data is aggregated together. This selection\n",
    "# is only valid for survey-area reports or other aggregated data\n",
    "# that may have survey results from both lakes and rivers.\n",
    "fd_kwargs ={\n",
    "    \"filename\": \"resources/checked_sdata_eos_2020_21x.csv\",\n",
    "    \"feature_name\": this_feature['slug'], \n",
    "    \"feature_level\": this_feature['level'], \n",
    "    \"these_features\": this_feature['slug'], \n",
    "    \"component\": this_level, \n",
    "    \"columns\": columns, \n",
    "    \"language\": 'de', \n",
    "    \"unit_label\": unit_label, \n",
    "    \"fail_rate\": fail_rate,\n",
    "    \"code_data\":dfCodes,\n",
    "    \"date_range\": start_end,\n",
    "    \"water_type\": None,    \n",
    "}\n",
    "\n",
    "fdx = featuredata.Components(**fd_kwargs)\n",
    "\n",
    "# call the reports and languages\n",
    "fdx.adjustForLanguage()\n",
    "fdx.makeFeatureData()\n",
    "fdx.locationSampleTotals()\n",
    "fdx.makeDailyTotalSummary()\n",
    "fdx.materialSummary()\n",
    "fdx.mostCommon()\n",
    "fdx.codeGroupSummary()\n",
    "\n",
    "# !this is the feature data!\n",
    "fd = fdx.feature_data\n",
    "\n",
    "# !keyword args to build period data\n",
    "# the period data is all the data that was collected\n",
    "# during the same period from all the other locations\n",
    "# not included in the feature data. For a survey area\n",
    "# or river bassin these_features = feature_parent and \n",
    "# feature_level = parent_level\n",
    "period_kwargs = {\n",
    "    \"period_data\": fdx.period_data,\n",
    "    \"these_features\": this_feature['slug'],\n",
    "    \"feature_level\":this_feature['level'],\n",
    "    \"feature_parent\":this_bassin,\n",
    "    \"parent_level\": \"river_bassin\",\n",
    "    \"period_name\": bassin_label,\n",
    "    \"unit_label\": unit_label,\n",
    "    \"most_common\": fdx.most_common.index\n",
    "}\n",
    "period_data = featuredata.PeriodResults(**period_kwargs)\n",
    "\n",
    "# collects the summarized values for the feature data\n",
    "# use this to generate the summary data for the survey area\n",
    "# and the section for the rivers\n",
    "admin_kwargs = {\n",
    "    \"data\":fd,\n",
    "    \"dims_data\":dfDims,\n",
    "    \"label\": this_feature[\"name\"],\n",
    "    \"feature_component\": this_level,\n",
    "    \"date_range\":start_end,\n",
    "    **{\"dfBeaches\":dfBeaches}\n",
    "}\n",
    "admin_details = featuredata.AdministrativeSummary(**admin_kwargs)\n",
    "admin_summary = admin_details.summaryObject()\n",
    "\n",
    "\n",
    "# calling componentsMostCommon gets the results for the most common codes\n",
    "# at the component level\n",
    "components = fdx.componentMostCommonPcsM()\n",
    "\n",
    "# map to proper names for features\n",
    "feature_names = admin_details.makeFeatureNameMap()\n",
    "\n",
    "# pivot that and quash the hierarchal column index that is created when the table is pivoted\n",
    "mc_comp = components[[\"item\", unit_label, this_level]].pivot(columns=this_level, index=\"item\")\n",
    "mc_comp.columns = mc_comp.columns.get_level_values(1)\n",
    "\n",
    "# insert the proper columns names for display\n",
    "proper_column_names = {x : feature_names.loc[x, 'water_name'] for x in mc_comp.columns}\n",
    "mc_comp.rename(columns = proper_column_names, inplace=True)\n",
    "\n",
    "# the aggregated total of the feature is taken from the most common objects table\n",
    "mc_feature = fdx.most_common[unit_label]\n",
    "mc_feature = featuredata.changeSeriesIndexLabels(mc_feature, {x:fdx.dMap.loc[x] for x in mc_feature.index})\n",
    "\n",
    "# the aggregated totals of all the period data\n",
    "mc_period = period_data.parentMostCommon(parent=False)\n",
    "mc_period = featuredata.changeSeriesIndexLabels(mc_period, {x:fdx.dMap.loc[x] for x in mc_period.index})\n",
    "\n",
    "# add the feature, bassin_label and period results to the components table\n",
    "mc_comp[this_feature[\"name\"]]= mc_feature\n",
    "mc_comp[top] = mc_period\n",
    "\n",
    "# caption_prefix =  f'Median {unit_label} der häufigsten Objekte am '\n",
    "# col_widths=[4.5*cm, *[1*cm]*(len(mc_comp.columns))]\n",
    "# mc_heatmap_title = Paragraph(\"Die am häufigsten gefundenen Objekte nach Gewässer\", subsection_title)\n",
    "# tables = featuredata.splitTableWidth(mc_comp, gradient=True, caption_prefix=caption_prefix, caption=mc_heat_map_caption,\n",
    "#                     this_feature=this_feature[\"name\"], vertical_header=True, colWidths=col_widths)\n",
    "\n",
    "# # identify the tables variable as either a list or a Flowable:\n",
    "# if isinstance(tables, (list, np.ndarray)):\n",
    "#     grouped_pdf_components = [*tables]\n",
    "# else:\n",
    "#     grouped_pdf_components = [tables]\n",
    "    \n",
    "\n",
    "# new_components = [\n",
    "#     small_space,\n",
    "#     mc_heatmap_title,\n",
    "#     small_space,\n",
    "#     *grouped_pdf_components\n",
    "# ]\n",
    "\n",
    "# pdfcomponents = addToDoc(new_components, pdfcomponents)\n",
    "\n",
    "# notebook display style\n",
    "aformatter = {x: featuredata.replaceDecimal for x in mc_comp.columns}\n",
    "\n",
    "mcd = mc_comp.style.format(aformatter).set_table_styles(heat_map_css_styles)\n",
    "mcd = mcd.background_gradient(axis=None, vmin=mc_comp.min().min(), vmax=mc_comp.max().max(), cmap=\"YlOrBr\")\n",
    "\n",
    "# remove the index name and column name labels\n",
    "mcd.index.name = None\n",
    "mcd.columns.name = None\n",
    "\n",
    "# rotate the text on the header row\n",
    "# the .applymap_index method in the\n",
    "# df.styler module is used for this\n",
    "mcd = mcd.applymap_index(featuredata.rotateText, axis=1)\n",
    "\n",
    "# # display markdown html\n",
    "# glue(f'{this_feature[\"slug\"]}_mc_heat_map_caption', mc_heat_map_caption, display=False)\n",
    "\n",
    "glue('baseline_most_aare_common_heat_map', mcd, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Umsetzung\n",
    "\n",
    "Die vorgeschlagenen Bewertungsmassstäbe und -methoden für die Ergebnisse der Untersuchungen der Strand-Abfallaufkommen sind ähnlich und kompatibel mit den zuvor in der Schweiz angewandten Methoden. Diese erste Analyse hat gezeigt:\n",
    "\n",
    "* dass die von der EU vorgeschlagenen Methoden zur Überwachung von Abfallobjekten in der Schweiz anwendbar sind,\n",
    "* dass Konfidenzintervalle und Basislinien für verschiedene Erhebungsgebiete berechnet werden können und\n",
    "* dass aggregierte Ergebnisse zwischen Regionen verglichen werden können.\n",
    "\n",
    "Sobald der BV für ein Erhebungsgebiet berechnet ist, können alle Proben, die innerhalb dieses Erhebungsgebiets durchgeführt werden, direkt mit diesem verglichen werden. Es zeigt sich, dass die Erhebungsgebiete in der Schweiz unterschiedliche Median-Basislinien haben. Diese Situation ist mit der EU vergleichbar, was die Unterschiede zwischen den verschiedenen Regionen und Verwaltungsgebieten betrifft.\n",
    "\n",
    "Durch die Anwendung der vorgeschlagenen Methoden auf die aktuellen Ergebnisse von IQAASL können die zu untersuchenden Objekte für jedes Untersuchungsgebiet identifiziert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{glue:figure} baselines_most_common_heat_map\n",
    "---\n",
    "name: baselines_most_common_heat_map\n",
    "---\n",
    "{glue:text}`blank_caption`  \n",
    "```\n",
    "{numref}`Abbildung %s: <baselines_most_common_heat_map>` Vergleich der BVs der häufigsten Objekte. Alle Datenerhebungen 2020–2021. Das Erhebungsgebiet Ticino/Ceresio hat weniger als 100 Datenerhebungen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Der erwartete Medianwert pro Datenerhebung und der Medianwert der häufigsten Objekte pro Erhebung ist im Erhebungsgebiet Rhône höher. Wenn der Medianwert verwendet wird, zeigt der BV auch, dass 2/12 der häufigsten Objekte in weniger als 50 % der Datenerhebungen landesweit gefunden wurden, nämlich diejenigen mit einem Medianwert von Null.\n",
    "\n",
    "Die Methode kann vertikal skaliert werden, um eine detailliertere Ansicht eines Erhebungsgebiets zu erhalten. Die Berechnungsmethode bleibt dieselbe, daher sind Vergleiche vom See bis zur nationalen Ebene möglich. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{glue:figure} baseline_most_aare_common_heat_map\n",
    "---\n",
    "name: baseline_most_aare_common_heat_map\n",
    "---\n",
    "{glue:text}`blank_caption` \n",
    "```\n",
    "{numref}`Abbildung {number}: <baseline_most_aare_common_heat_map>` Vergleich der Basiswerte der häufigsten Objekte. Erhebungsgebiet Aare, Seen und Fliessgewässer 2020–2021. Orte mit mehr als 30 Datenerhebungen: Bielersee, Neuenburgersee und Thunersee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "implement_title = Paragraph(\"Umsetzung\", section_title)\n",
    "\n",
    "p_50 = [\n",
    "    \"Die vorgeschlagenen Bewertungsmassstäbe und -methoden für die Ergebnisse der \",\n",
    "    \"Untersuchungen der Strand-Abfallaufkommen sind ähnlich und kompatibel mit \",\n",
    "    \"den zuvor in der Schweiz angewandten Methoden. Diese erste Analyse hat gezeigt:\"\n",
    "]\n",
    "p_50 =makeAParagraph(p_50)\n",
    "\n",
    "implement_list = [\n",
    "    \"dass die von der EU vorgeschlagenen Methoden zur Überwachung von Abfallobjekten in der Schweiz anwendbar sind\",\n",
    "    \"dass Konfidenzintervalle und Basislinien für verschiedene Erhebungsgebiete berechnet werden können und\",\n",
    "    \"dass aggregierte Ergebnisse zwischen Regionen verglichen werden können.\"\n",
    "]\n",
    "implement_list = makeAList(implement_list)\n",
    "\n",
    "p_51 =[\n",
    "    \"Sobald der BV für ein Erhebungsgebiet berechnet ist, können alle Proben, die innerhalb dieses Erhebungsgebiets \",\n",
    "    \"durchgeführt werden, direkt mit diesem verglichen werden. Es zeigt sich, dass die Erhebungsgebiete in der Schweiz \",\n",
    "    \"unterschiedliche Median-Basislinien haben. Diese Situation ist mit der EU vergleichbar, \",\n",
    "    \"was die Unterschiede zwischen den verschiedenen Regionen und Verwaltungsgebieten betrifft.\"\n",
    "]\n",
    "\n",
    "p_52 = [\n",
    "    \"Durch die Anwendung der vorgeschlagenen Methoden auf die aktuellen Ergebnisse von IQAASL \",\n",
    "    \"können die zu untersuchenden Objekte für jedes Untersuchungsgebiet identifiziert werden.\"\n",
    "]\n",
    "\n",
    "p_51_52 = sectionParagraphs([p_51, p_52], smallspace=smallest_space)\n",
    "\n",
    "fig_16_cap = [\n",
    "    \"Vergleich der BVs der häufigsten Objekte. Alle Datenerhebungen 2020–2021. \",\n",
    "    \"Das Erhebungsgebiet Ticino/Ceresio hat weniger als 100 Datenerhebungen.\"\n",
    "]\n",
    "\n",
    "fig_16_cap = makeAParagraph(fig_16_cap, caption_style)\n",
    "\n",
    "figure_kwargs.update({\n",
    "    \"image_file\":\"resources/images/baselines/lakes_rivers_de_22_0.png\",\n",
    "    \"original_width\":20.1,\n",
    "    \"original_height\":27.055,\n",
    "    \"desired_width\": 10,\n",
    "    \"caption\":fig_16_cap,\n",
    "    \"caption_height\":1.5,\n",
    "})\n",
    "\n",
    "figure_16 = figureAndCaptionTable(**figure_kwargs)\n",
    "\n",
    "p_53 = [\n",
    "    \"Der erwartete Medianwert pro Datenerhebung und der Medianwert der häufigsten Objekte pro Erhebung ist im Erhebungsgebiet Rhône höher. \",\n",
    "    \"Wenn der Medianwert verwendet wird, zeigt der BV auch, dass 2/12 der häufigsten Objekte in weniger als 50 % der Datenerhebungen landesweit \",\n",
    "    \"gefunden wurden, nämlich diejenigen mit einem Medianwert von Null.\"\n",
    "]\n",
    "\n",
    "p_54 = [\n",
    "    \"Die Methode kann vertikal skaliert werden, um eine detailliertere Ansicht eines Erhebungsgebiets zu erhalten. \",\n",
    "    \"Die Berechnungsmethode bleibt dieselbe, daher sind Vergleiche vom See bis zur nationalen Ebene möglich.\"\n",
    "]\n",
    "\n",
    "p_53_54 = sectionParagraphs([p_53, p_54], smallspace=smallest_space)\n",
    "\n",
    "fig_17_cap = [\n",
    "    \"Vergleich der Basiswerte der häufigsten Objekte. Erhebungsgebiet Aare, Seen und Fliessgewässer 2020–2021. \",\n",
    "    \"Orte mit mehr als 30 Datenerhebungen: Bielersee, Neuenburgersee und Thunersee.\"\n",
    "]\n",
    "\n",
    "fig_17_cap = makeAParagraph(fig_17_cap, caption_style)\n",
    "\n",
    "figure_kwargs.update({\n",
    "    \"image_file\":\"resources/images/baselines/aare_sa_de_23_0.png\",\n",
    "    \"original_width\":30.1,\n",
    "    \"original_height\":25.6,\n",
    "    \"desired_width\": 16,\n",
    "    \"caption\":fig_17_cap,\n",
    "    \"caption_height\":1.5,\n",
    "})\n",
    "\n",
    "figure_17 = figureAndCaptionTable(**figure_kwargs)\n",
    "\n",
    "new_components = [\n",
    "    implement_title,\n",
    "    small_space,\n",
    "    p_50,\n",
    "    KeepTogether(smallest_space),\n",
    "    implement_list,\n",
    "    KeepTogether(smallest_space),\n",
    "    *p_51_52,\n",
    "    figure_16,\n",
    "    KeepTogether(smallest_space),\n",
    "    *p_53_54,\n",
    "    figure_17,    \n",
    "]\n",
    "\n",
    "pdfcomponents = addToDoc(new_components, pdfcomponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Die empfohlene Mindestanzahl von Datenerhebungen (40) pro Probenahmezeitraum soll sicherstellen, dass die BV-Berechnungen auf einer ausreichenden Anzahl von Stichproben basieren. Dies ist wegen der hohen Variabilität der Untersuchungen der Strand-Abfallobjekte wichtig.\n",
    "\n",
    "Die Probenahmen für IQAASL haben im Zeitraum von April 2020 bis Mai 2021 stattgefunden. Unter Berücksichtigung der Mindestanzahl der Proben gibt es drei Basiswerte für das Erhebungsgebiet Aare: \n",
    "\n",
    "* Bielersee\n",
    "* Neuenburgersee\n",
    "* Erhebungsgebiet Aare  \n",
    "\n",
    "Für Bewertungszwecke bedeutet dies, dass eine Stichprobe als Stichprobenbewertung dienen kann und die Ergebnisse direkt mit einer der regionalen Basislinien verglichen werden können, was ein sofortiges Feedback ermöglicht. Diese Art der Bewertung vereinfacht den Prozess und versetzt die lokalen Akteure in die Lage, unabhängige Bewertungen vorzunehmen, Schlussfolgerungen zu ziehen und auf der Grundlage der Ergebnisse der festgelegten BVs für das Erhebungsgebiet Minderungsstrategien festzulegen.\n",
    "\n",
    "In den vorherigen Beispielen sind keine Schwellenwerte oder Extremwerte angegeben. Werte, die grösser als Null sind, entsprechen dem erwarteten Medianwert des Objekts für jede gemessene Einheit. Ein Nullwert bedeutet, dass das Objekt in weniger als 50 % der Datenerhebungen gefunden wurde. Der Perzentil-Rang für ein bestimmtes Objekt lässt sich ableiten, indem die Wertetabelle in horizontaler Richtung gelesen wird. \n",
    "\n",
    "Wie aussagekräftig diese Ergebnisse für die Bewertung von Minderungsstrategien sind, hängt von der Anzahl und Qualität der Proben ab. Interessengruppen auf kommunaler oder lokaler Ebene benötigen detaillierte Daten über bestimmte Objekte. Nationale und internationale Stakeholder hingegen tendieren dazu, breitere, aggregierte Gruppen zu verwenden. \n",
    "\n",
    "Die Qualität der Daten steht in direktem Zusammenhang mit der Ausbildung und Unterstützung der Personen, die die Datenerhebung durchführen. Der Identifizierungsprozess erfordert ein gewisses Mass an Fachwissen, da viele Objekte und Materialien dem Durchschnittsbürger nicht bekannt sind. Ein Kernteam von erfahrenen Personen, die bei der Entwicklung und Schulung helfen, stellt sicher, dass die Datenerhebungen im Laufe der Zeit konsistent durchgeführt werden. \n",
    "\n",
    "Das Monitoring-Programm in der Schweiz hat es geschafft, mit den Entwicklungen auf dem Kontinent Schritt zu halten, es gibt jedoch viele Bereiche, die verbessert werden können:\n",
    "\n",
    "1. Festlegung einer standardisierten Methode zur Berichterstattung für kommunale, kantonale und nationale Akteure\n",
    "2. Definition von Monitoring- oder Bewertungszielen\n",
    "3. Formalisierung des Datenspeichers und der Methode zur Implementierung auf verschiedenen Verwaltungsebenen\n",
    "4. Aufbau eines Netzwerks von Verbänden, die sich die Verantwortung und die Ressourcen für die Vermessung des Gebiets teilen\n",
    "5. Entwicklung und Implementierung eines formellen Schulungsprogramms für die Personen, welche die Datenerhebung ausführen\n",
    "6. Ermittlung der idealen Stichproben-Szenarien und des Forschungsbedarfs mit akademischen Partnern\n",
    "7. Entwicklung einer Finanzierungsmethode zur Durchführung der empfohlenen Mindestanzahl von Erhebungen (40) pro Erhebungszeitraum und Erhebungsgebiet, um sicherzustellen, dass genaue Bewertungen vorgenommen werden können und die Forschungsanforderungen erfüllt werden.\n",
    "\n",
    "Veränderungen in den Ergebnissen von Strand-Abfalluntersuchungen sind Signale, und die Verwendung von Basiswerten hilft, das Ausmass dieser Signale zu erkennen. Ohne Kontext oder zusätzliche Informationen können diese Signale jedoch zufällig erscheinen. \n",
    "\n",
    "Zum Expertenwissen gehört die Fähigkeit, Erhebungsergebnisse in den Kontext lokaler Ereignisse und der Topographie einzuordnen. Dieses Urteilsvermögen in Bezug auf die Daten und die Umgebung ist für die Identifizierung potenzieller Quellen und Prioritäten von wesentlicher Bedeutung. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "p_55 = [\n",
    "    \"Die empfohlene Mindestanzahl von Datenerhebungen (40) pro Probenahmezeitraum soll sicherstellen, dass die BV-Berechnungen auf einer \",\n",
    "    \"ausreichenden Anzahl von Stichproben basieren. Dies ist wegen der hohen Variabilität der Untersuchungen der Strand-Abfallobjekte wichtig.\"\n",
    "]\n",
    "\n",
    "p_56 = [\n",
    "    \"Die Probenahmen für IQAASL haben im Zeitraum von April 2020 bis Mai 2021 stattgefunden. Unter Berücksichtigung der \",\n",
    "    \"Mindestanzahl der Proben gibt es drei Basiswerte für das Erhebungsgebiet Aare: \"\n",
    "]\n",
    "\n",
    "p_55_56 = sectionParagraphs([p_55, p_56], smallspace=smallest_space)\n",
    "\n",
    "lake_list = [\n",
    "    \"Bielersee\",\n",
    "    \"Neuenburgersee\",\n",
    "    \"Erhebungsgebiet Aare\"\n",
    "]\n",
    "\n",
    "lake_list = makeAList(lake_list)\n",
    "\n",
    "p_57 = [\n",
    "    \"Für Bewertungszwecke bedeutet dies, dass eine Stichprobe als Stichprobenbewertung dienen kann und \",\n",
    "    \"die Ergebnisse direkt mit einer der regionalen Basislinien verglichen werden können, was ein \",\n",
    "    \"sofortiges Feedback ermöglicht. Diese Art der Bewertung vereinfacht den Prozess und versetzt die \",\n",
    "    \"lokalen Akteure in die Lage, unabhängige Bewertungen vorzunehmen, Schlussfolgerungen zu ziehen und \",\n",
    "    \"auf der Grundlage der Ergebnisse der festgelegten BVs für das Erhebungsgebiet Minderungsstrategien festzulegen.\"\n",
    "]\n",
    "p_57 = ''.join(p_57)\n",
    "\n",
    "p_57 = Paragraph(p_57, style=featuredata.p_style)\n",
    "\n",
    "p_58 = [\n",
    "    \"In den vorherigen Beispielen sind keine Schwellenwerte oder Extremwerte angegeben. Werte, die grösser als \",\n",
    "    \"Null sind, entsprechen dem erwarteten Medianwert des Objekts für jede gemessene Einheit. Ein Nullwert bedeutet, \",\n",
    "    \"dass das Objekt in weniger als 50 % der Datenerhebungen gefunden wurde. Der Perzentil-Rang für ein bestimmtes \",\n",
    "    \"Objekt lässt sich ableiten, indem die Wertetabelle in horizontaler Richtung gelesen wird.\"\n",
    "]\n",
    "p_58 = makeAParagraph(p_58)\n",
    "\n",
    "p_59 = [\n",
    "    \"Wie aussagekräftig diese Ergebnisse für die Bewertung von Minderungsstrategien sind, hängt von der Anzahl und \",\n",
    "    \"Qualität der Proben ab. Interessengruppen auf kommunaler oder lokaler Ebene benötigen detaillierte Daten über \",\n",
    "    \"bestimmte Objekte. Nationale und internationale Stakeholder hingegen tendieren dazu, breitere, \",\n",
    "    \"aggregierte Gruppen zu verwenden.\"\n",
    "]\n",
    "\n",
    "p_59 = makeAParagraph(p_59)\n",
    "\n",
    "p_60 = [\n",
    "    \"Die Qualität der Daten steht in direktem Zusammenhang mit der Ausbildung und Unterstützung der Personen, die die \",\n",
    "    \"Datenerhebung durchführen. Der Identifizierungsprozess erfordert ein gewisses Mass an Fachwissen, da viele Objekte \",\n",
    "    \"und Materialien dem Durchschnittsbürger nicht bekannt sind. Ein Kernteam von erfahrenen Personen, die bei der \",\n",
    "    \"Entwicklung und Schulung helfen, stellt sicher, dass die Datenerhebungen im Laufe der Zeit konsistent durchgeführt werden.\"\n",
    "]\n",
    "\n",
    "p_61 = [\n",
    "    \"Das Monitoring-Programm in der Schweiz hat es geschafft, mit den Entwicklungen auf dem Kontinent Schritt zu halten, \", \n",
    "    \"es gibt jedoch viele Bereiche, die verbessert werden können:\"\n",
    "]\n",
    "\n",
    "# p_57_59 = sectionParagraphs([p_57, p_58, p_59], smallspace=smallest_space)\n",
    "\n",
    "p_60_61 = sectionParagraphs([p_60, p_61],smallspace=smallest_space)\n",
    "last_list = [\n",
    "    \"Festlegung einer standardisierten Methode zur Berichterstattung für kommunale, kantonale und nationale Akteure\",\n",
    "    \"Definition von Monitoring- oder Bewertungszielen\",\n",
    "    \"Formalisierung des Datenspeichers und der Methode zur Implementierung auf verschiedenen Verwaltungsebenen\",\n",
    "    \"Aufbau eines Netzwerks von Verbänden, die sich die Verantwortung und die Ressourcen für die Vermessung des Gebiets teilen\",\n",
    "    \"Entwicklung und Implementierung eines formellen Schulungsprogramms für die Personen, welche die Datenerhebung ausführen\",\n",
    "    \"Ermittlung der idealen Stichproben-Szenarien und des Forschungsbedarfs mit akademischen Partnern\",\n",
    "    \"Entwicklung einer Finanzierungsmethode zur Durchführung der empfohlenen Mindestanzahl von Erhebungen (40) pro Erhebungszeitraum und Erhebungsgebiet, \",\n",
    "    \"um sicherzustellen, dass genaue Bewertungen vorgenommen werden können und die Forschungsanforderungen erfüllt werden.\"\n",
    "]\n",
    "\n",
    "last_list = makeAList(last_list)\n",
    "\n",
    "p_62 = [\n",
    "    \"Veränderungen in den Ergebnissen von Strand-Abfalluntersuchungen sind Signale, und die Verwendung von Basiswerten hilft, \",\n",
    "    \"das Ausmass dieser Signale zu erkennen. Ohne Kontext oder zusätzliche Informationen können diese Signale jedoch zufällig erscheinen.\"\n",
    "]\n",
    "\n",
    "p_63 = [\n",
    "    \"Zum Expertenwissen gehört die Fähigkeit, Erhebungsergebnisse in den Kontext lokaler Ereignisse und der Topographie einzuordnen. \",\n",
    "    \"Dieses Urteilsvermögen in Bezug auf die Daten und die Umgebung ist für die Identifizierung potenzieller Quellen und Prioritäten von wesentlicher Bedeutung.\"\n",
    "]\n",
    "\n",
    "p_62_63 = sectionParagraphs([p_62, p_63], smallspace=smallest_space)\n",
    "\n",
    "new_components = [\n",
    "    *p_55_56,\n",
    "    lake_list,\n",
    "    KeepTogether(smallest_space),\n",
    "    p_57,    \n",
    "    KeepTogether(smallest_space),\n",
    "    p_58,\n",
    "    KeepTogether(smallest_space),\n",
    "    p_59,\n",
    "    KeepTogether(smallest_space),\n",
    "    *p_60_61,\n",
    "    smallest_space,\n",
    "    last_list,\n",
    "    smallest_space,\n",
    "    *p_62_63, \n",
    "    *references\n",
    "]\n",
    "\n",
    "pdfcomponents = addToDoc(new_components, pdfcomponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "doc = SimpleDocTemplate(\"resources/pdfs/baselines.pdf\", pagesize=A4, leftMargin=2.5*cm, rightMargin=2.5*cm, topMargin=2.5*cm, bottomMargin=1.5*cm)\n",
    "pageinfo= f\"IQAASL/Anwendungen/Basiswerte für Abfallobjekte an Gewässern\"\n",
    "\n",
    "source_prefix = \"https://hammerdirt-analyst.github.io/IQAASL-End-0f-Sampling-2021/\"\n",
    "source = \"baselines.html\"\n",
    "\n",
    "link_to_source = f'{source_prefix}{source}'\n",
    "\n",
    "def myLaterPages(canvas, doc):\n",
    "    canvas.saveState()\n",
    "    canvas.setLineWidth(.001*cm)\n",
    "    canvas.setFillAlpha(.8)\n",
    "    canvas.line(2.5*cm, 27.6*cm,  18.5*cm, 27.6*cm) \n",
    "    canvas.setFont('Times-Roman',9)\n",
    "    canvas.drawString(2.5*cm, 1*cm, link_to_source)\n",
    "    canvas.drawString(18.5*cm, 1*cm,  \"S.%d \" % (doc.page,))\n",
    "    canvas.drawString(2.5*cm, 27.7*cm, pageinfo)\n",
    "    canvas.restoreState()\n",
    "    \n",
    "doc.build(pdfcomponents,  onFirstPage=myLaterPages, onLaterPages=myLaterPages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}