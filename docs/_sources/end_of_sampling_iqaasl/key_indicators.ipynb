{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# sys, file and nav packages:\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "# math packages:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# charting:\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import ticker\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "# home brew utitilties\n",
    "import resources.utility_functions as ut\n",
    "import resources.abundance_classes as ac\n",
    "import resources.chart_kwargs as ck\n",
    "\n",
    "import resources.sr_ut as sut\n",
    "\n",
    "# images and display\n",
    "import base64, io, IPython\n",
    "from PIL import Image as PILImage\n",
    "from IPython.display import Markdown as md\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "# set some parameters:\n",
    "today = dt.datetime.now().date().strftime(\"%Y-%m-%d\")\n",
    "start_date = '2020-03-01'\n",
    "end_date ='2021-05-31'\n",
    "\n",
    "a_fail_rate = 50\n",
    "\n",
    "# the city, lake and river bassin we are aggregating to\n",
    "# the keys are column names in the survey data\n",
    "levels = {\"city\":\"Biel/Bienne\",\"water_name_slug\":'bielersee', \"river_bassin\":'aare'}\n",
    "level_names = [levels['city'], \"Bielersee\",\"Aare survey area\"]\n",
    "\n",
    "# name of the output folder:\n",
    "name_of_project = 'key_indicators_report'\n",
    "\n",
    "# colors for gradients\n",
    "colors = ['beige', 'navajowhite', 'sandybrown', 'salmon', 'sienna']\n",
    "nodes = [0.0, 0.2, 0.6, 0.8, 1.0]\n",
    "cmap2 = LinearSegmentedColormap.from_list(\"mycmap\", list(zip(nodes, colors)))\n",
    "\n",
    "# add the folder to the directory tree:\n",
    "project_directory = ut.make_project_folder('output', name_of_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key statistical indicators\n",
    "\n",
    "The key indicators provide answers to the most frequent questions about the state of litter in the natural environment. The key indicators are:\n",
    "\n",
    "1. number of samples\n",
    "2. pass-fail rate (fail-rate)\n",
    "3. pieces of trash per meter (pcs/m or pcs/mÂ²)\n",
    "4. percent of total trash (% of total)\n",
    "\n",
    "Easy to calculate and taken directly from the survey results the key indicators are essential to identifying zones of accumulation within the watershed. When combined with specific knowledge of the surrounding area the key indicators help identify potential sources.{cite}`mlwguidance`\n",
    "\n",
    "__Indicators for the most frequent questions__\n",
    "\n",
    "Assessments of beach-litter surveys describe the location, abundance and composition of the objects found {cite}`eubaselines`. The key indicators answer the following questions:\n",
    "\n",
    "*  What items are found?\n",
    "   * The most abundant objects from the water feature and the survey area sorted by total object count \n",
    "*  How much is found ? (total weights and item counts)\n",
    "   * Total object count, total weight and weight of plastics\n",
    "*  How often are these items found?\n",
    "   * How often the most abundant items were identified\n",
    "*  Where are these items found in the greatest concentration?\n",
    "   * Pieces of trash per meter (pcs/m): the ratio of number of objects found to the length of the shoreline for each survey location\n",
    "\n",
    "These questions could apply to almost every environmental monitoring procedure that rely on observations to estimate populations. Examples include the populations of migrating birds or the prescence of native plant species. Similar to counting birds or wildflowers a litter-surveyor has to find the quarry and then identify it, this process is well documented and has been tested under many conditions{cite}`Ryan2015` {cite}`Rech`.\n",
    "\n",
    "__Assumptions of the key indicators:__\n",
    "\n",
    "The reliability of these indicators is based on the following assumptions:\n",
    "\n",
    "1. The more trash there is on the ground the more a person is likely to find\n",
    "2. The survey results represent the minimum amount of trash at that site\n",
    "3. The surveyors are following the protocol and recording findings accurately\n",
    "4. For each survey: finding one item does not effect the chance of finding another {cite}`iid`\n",
    "\n",
    "__Using the key indicators__\n",
    "\n",
    "The key indicators of the most common objects are given with every data summary at each aggregation level. If the previous assumptions are maintained then the number of samples in the region of interest should be considered first before applying any weight to conclusions that may be drawn from the survey results. While one individual sample can give very specific data about one place and date, a collection of individual samples speaks to the region in general. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# get your data:\n",
    "survey_data = pd.read_csv('resources/results_with_land_use_2015.csv')\n",
    "river_bassins = ut.json_file_get(\"resources/river_basins.json\")\n",
    "dfBeaches = pd.read_csv(\"resources/beaches_with_land_use_rates.csv\")\n",
    "dfCodes = pd.read_csv(\"resources/codes_with_group_names_2015.csv\")\n",
    "dfDims = pd.read_csv(\"resources/dims_data.csv\")\n",
    "\n",
    "# set the index of the beach data to location slug\n",
    "dfBeaches.set_index('slug', inplace=True)\n",
    "\n",
    "# map locations to feature names\n",
    "location_wname_key = dfBeaches.water_name_slug\n",
    "\n",
    "# map water_name_slug to water_name\n",
    "wname_wname = dfBeaches[['water_name_slug','water_name']].reset_index(drop=True).drop_duplicates()\n",
    "wname_wname.set_index('water_name_slug', inplace=True)\n",
    "\n",
    "# make changes to code def for display\n",
    "dfCodes.set_index(\"code\", inplace=True)\n",
    "\n",
    "# these descriptions need to be shortened for display\n",
    "dfCodes = sut.shorten_the_value([\"G74\", \"description\", \"Insulation: includes spray foams\"], dfCodes)\n",
    "dfCodes = sut.shorten_the_value([\"G940\", \"description\", \"Foamed EVA for crafts and sports\"], dfCodes)\n",
    "dfCodes = sut.shorten_the_value([\"G96\", \"description\", \"Sanitary-pads/tampons, applicators\"], dfCodes)\n",
    "dfCodes = sut.shorten_the_value([\"G178\", \"description\", \"Metal bottle caps and lids\"], dfCodes)\n",
    "dfCodes = sut.shorten_the_value([\"G82\", \"description\", \"Expanded foams 2.5cm - 50cm\"], dfCodes)\n",
    "dfCodes = sut.shorten_the_value([\"G81\", \"description\", \"Expanded foams .5cm - 2.5cm\"], dfCodes)\n",
    "dfCodes = sut.shorten_the_value([\"G117\", \"description\", \"Expanded foams < 5mm\"], dfCodes)\n",
    "dfCodes = sut.shorten_the_value([\"G75\", \"description\", \"Plastic/foamed polystyrene 0 - 2.5cm\"], dfCodes)\n",
    "dfCodes = sut.shorten_the_value([\"G76\", \"description\", \"Plastic/foamed polystyrene 2.5cm - 50cm\"], dfCodes)\n",
    "dfCodes = sut.shorten_the_value([\"G24\", \"description\", \"Plastic lid rings\"], dfCodes)\n",
    "dfCodes = sut.shorten_the_value([\"G33\", \"description\", \"Lids for togo drinks plastic\"], dfCodes)\n",
    "dfCodes = sut.shorten_the_value([\"G3\", \"description\", \"Plastic bags, carier bags\"], dfCodes)\n",
    "dfCodes = sut.shorten_the_value([\"G204\", \"description\", \"Bricks, pipes not plastic\"], dfCodes)\n",
    "\n",
    "# make a map to the code descriptions\n",
    "code_description_map = dfCodes.description\n",
    "\n",
    "# make a map to the code descriptions\n",
    "code_material_map = dfCodes.material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The data for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'city'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5314/87843162.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# the city that we are looking at:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mbiel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcity\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'city'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# samples at biel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iqaasl/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5463\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5464\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5465\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'city'"
     ]
    }
   ],
   "source": [
    "dfSurveys = ac.fo_rmat_and_slice_date(survey_data.copy(), a_format=\"%Y-%m-%d\", start_date=start_date, end_date=end_date)\n",
    "\n",
    "trb = dfSurveys[dfSurveys.river_bassin == levels['river_bassin']].copy()\n",
    "\n",
    "# describe the data set:\n",
    "num_obs = len(trb)\n",
    "num_samps = len(trb.loc_date.unique())\n",
    "num_obj = trb.quantity.sum()\n",
    "num_locs = len(trb.location.unique())\n",
    "\n",
    "# the city that we are looking at:\n",
    "biel = trb[trb.city == levels['city']]\n",
    "\n",
    "# samples at biel\n",
    "biel_locd = biel.loc_date.unique()\n",
    "\n",
    "# locations at biel\n",
    "biel_loc = biel.location.unique()\n",
    "\n",
    "# example data summary and keys\n",
    "biel_t = biel.quantity.sum()\n",
    "biel_fail = biel.loc[biel.quantity > 0]\n",
    "biel_nfail = len(biel_fail.code.unique())\n",
    "\n",
    "# print(F\"\\n Results for all surveys between {start_date} and {end_date} from the following catchment areas:\\n\\n  {trb.river_bassin.unique()}\")\n",
    "md(F\"\\n\\nThe data is from the Aare survey area between 2020-03-01 and 2020-05-31. There were {'{:,}'.format(num_obj)} objects collected from {num_samps} surveys.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "bassin_map = PILImage.open(\"resources/maps/aare_scaled.jpeg\")\n",
    "\n",
    "bassin_map.thumbnail((800, 1200))\n",
    "\n",
    "\n",
    "output = io.BytesIO()\n",
    "bassin_map.save(output, format='PNG')\n",
    "encoded_string = base64.b64encode(output.getvalue()).decode()\n",
    "\n",
    "html = '<img src=\"data:image/png;base64,{}\"/>'.format(encoded_string)\n",
    "IPython.display.HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# this is a convenience function for the abundance class\n",
    "# the fail rate needs to be recalculated at each aggregation level\n",
    "fail_rates_df = ac.agg_fail_rate_by_city_feature_basin_all(dfSurveys, levels, group='code')\n",
    "fail_rates_df['item'] = fail_rates_df.index.map(lambda x: ut.use_this_key(x, code_description_map))\n",
    "fail_rates_df.set_index('item', drop=True, inplace=True)\n",
    "\n",
    "# keep the list of top ten:\n",
    "the_top_ten = fail_rates_df.sort_values(by='Biel/Bienne',ascending=False)[:10].index\n",
    "\n",
    "# the labels for the summary table:\n",
    "unit_label= 'pcs_m'\n",
    "change_names = {'count':'# samples', \n",
    "                'mean':F\"average {unit_label}\",\n",
    "                'std':'standard deviation', \n",
    "                'min p/50m':'min', '25%':'25%',\n",
    "                '50%':'50%', '75%':'75%',\n",
    "                'max':F\"max {unit_label}\", 'min':F\"min {unit_label}\",\n",
    "                'total objects':'total objects',\n",
    "                '# locations':'# locations',\n",
    "                'survey year':'survey year'\n",
    "               }\n",
    "\n",
    "# convenience function to change the index names in a series\n",
    "def anew_dict(x):\n",
    "    new_dict = {}\n",
    "    for param in x.index:\n",
    "        new_dict.update({change_names[param]:x[param]})\n",
    "    return new_dict  \n",
    "\n",
    "# select data\n",
    "data = trb.groupby(['loc_date','location',  'date'], as_index=False).agg({'pcs_m':'sum', 'quantity':'sum'})\n",
    "\n",
    "# get the basic statistics from pd.describe\n",
    "desc_biel = data['pcs_m'].describe().round(2)\n",
    "\n",
    "# add project totals\n",
    "desc_biel['total objects'] = data.quantity.sum()\n",
    "desc_biel['# locations'] = data.location.nunique()\n",
    "\n",
    "# change the names\n",
    "combined_summary = pd.Series(anew_dict(desc_biel))\n",
    "\n",
    "# format the output for printing:\n",
    "not_formatted = combined_summary[-1]\n",
    "combined_summary = [(x, \"{:,}\".format(int(combined_summary[x]))) for x in combined_summary.index[:-1]]\n",
    "combined_summary.append((desc_biel.index[-1], int(not_formatted) ))\n",
    "\n",
    "# format for time series\n",
    "months = mdates.MonthLocator(interval=1)\n",
    "months_fmt = mdates.DateFormatter('%b')\n",
    "days = mdates.DayLocator(interval=7)\n",
    "\n",
    "# get the data\n",
    "dt_all = trb.groupby(['loc_date', 'date'], as_index=False).pcs_m.sum()\n",
    "monthly_plot = dt_all.set_index('date', drop=True).pcs_m.resample('M').median()\n",
    "\n",
    "fig = plt.figure(figsize=(11, 6))\n",
    "\n",
    "spec = GridSpec(ncols=8, nrows=2, figure=fig)\n",
    "axone = fig.add_subplot(spec[:,0:5])\n",
    "axtwo = fig.add_subplot(spec[:,5:8])\n",
    "\n",
    "sns.scatterplot(data=dt_all, x='date', y='pcs_m', color='black', alpha=0.2, label=\"All survey areas\", ax=axone)\n",
    "sns.lineplot(data=monthly_plot, x=monthly_plot.index, y=monthly_plot, color='magenta', label=F\"Monthly median:{level_names[2]}\", ax=axone)\n",
    "\n",
    "the_90th = np.percentile(dt_all.pcs_m, 99)\n",
    "\n",
    "not_included = F\"Values greater than the 99th percentile ({round(the_90th, 2)}) not shown.\"\n",
    "\n",
    "axone.set_ylabel(\"Pieces of trash per meter\", **ck.xlab_k14)\n",
    "axone.set_ylim(0,the_90th )\n",
    "axone.set_title(F\"{level_names[2]}, {start_date[:7]} through {end_date[:7]}, n={num_samps}\\n{not_included}\",  **ck.title_k)\n",
    "axone.xaxis.set_minor_locator(days)\n",
    "axone.xaxis.set_major_formatter(months_fmt)\n",
    "axone.set_xlabel(\"\")\n",
    "\n",
    "axtwo.set_xlabel(\" \")\n",
    "ut.hide_spines_ticks_grids(axtwo)\n",
    "\n",
    "a_col = ['Aare survey area', 'total']\n",
    "a_table = axtwo.table(cellText=combined_summary,  colLabels=a_col, colWidths=[.4, .3,.3], loc='lower center', bbox=[0,0,1,1])\n",
    "the_material_table_data = sut.make_a_summary_table(a_table,combined_summary,a_col, s_et_bottom_row=False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The key indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of samples\n",
    "\n",
    "The number of samples is the simplest and most important statistic to consider. The more samples there are the easier it is to spot irregularities or departures from the norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The fail rate: how often an object was found\n",
    "\n",
    "The pass-fail rate is the number of times that an object was found at least once divided by the number of surveys.\n",
    "\n",
    "**What does it mean?** The fail rate describes the percent of times that a category **was** identified in relation to the amount of surveys conducted.\n",
    "\n",
    "> Use the fail rate to determine how frequently an object is found within a geographic range. Items can be differentiated by the fail rate. Use the fail rate and pcs/m to identify objects that are found infrequently but in important quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Different fail rates at different levels__\n",
    "\n",
    "The fail rate can be calculated for any lake, municipality or river bassin _provided you have a sufficient quantity of reliable data_ for the location of interest.\n",
    "\n",
    "__Biel/Bienne the ten items with the highest fail rate__\n",
    "\n",
    "Compare the fail-rates of the ten most common items from the 16 surveys in Biel to the pass-fail-rates of those same items for Bielersee the Aare and all other survey areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# this is a convenience function for the abundance class\n",
    "# the fail rate needs to be recalculated at each aggregation level\n",
    "\n",
    "fail_rates_df = ac.agg_fail_rate_by_city_feature_basin_all(dfSurveys, levels, group='code')\n",
    "fail_rates_dfx = fail_rates_df.copy()\n",
    "fail_rates_dfx['item'] = fail_rates_dfx.index.map(lambda x: code_description_map.loc[x])\n",
    "fail_rates_dfx.set_index('item', drop=True, inplace=True)\n",
    "\n",
    "# keep the list of top ten:\n",
    "the_top_ten = fail_rates_df.sort_values(by='Biel/Bienne',ascending=False)[:10].index\n",
    "\n",
    "# plot that\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "# axone.tick_params(labelsize=14, which='both', axis='both')\n",
    "sns.heatmap(fail_rates_dfx.sort_values(by=\"Biel/Bienne\", ascending=False)[:10], cmap=cmap2, annot=True, annot_kws={'fontsize':12}, fmt='.0%', ax=ax, square=True, cbar=False, linewidth=.05,  linecolor='white')\n",
    "ax.tick_params(**ck.xlabels_top, **ck.no_xticks)\n",
    "\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_xlabel(\" \")\n",
    "plt.setp(ax.get_xticklabels(), rotation=90)\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the exception of industrial sheeting the fail rate was greater in Biel/Bienne than the rest of the lake, the river bassin and nationally. This means that, in general, there was a greater chance of finding those objects at Biel/Bienne than most other places.\n",
    "\n",
    "The pass-fail rate is the  most likely estimate (MLE) of the probability of finding at least one object {cite}`mle`. **A 100% fail rate does not mean that you are guaranteed to find the object**, it means that the object was identified in all previous samples and you should expect to find the object at the next survey. The inverse is also true for objects that have a lower fail rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pieces per meter: number of objects found per length of shoreline\n",
    "\n",
    "Pieces per meter is the number of objects found at each survey divided by the length of the survey.\n",
    "\n",
    "**What does it mean?** Pieces per meter describes the quantity of an object that was found for each meter of shoreline surveyed. Using pieces per meter allows survey results to be compared indifferent of the size of the survey.\n",
    "\n",
    "> Use pieces per meter to find the objects that were found in the greatest quantities. Use pieces per meter to identify zones of accumulation.\n",
    "\n",
    "_Why not use the surface area?_ The recomended EU standard is to report the results as quantity of objects per length of shoreline surveyed, usually 100 meters. Both survey dimensions were recorded at each survey for the specified dates. The units are reported in pieces of trash per meter.\n",
    "\n",
    "__Biel/Bienne: the ten objects with the highest median pieces per meter per survey__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "top_ten_pcsm = ac.agg_pcs_m_by_city_feature_basin_all(\n",
    "    dfSurveys,\n",
    "    levels,\n",
    "    level_names=level_names,\n",
    "    agg_cols={\"pcs_m\":\"median\"},\n",
    "    national=True,\n",
    "    **{\"bassin_summary\":False}\n",
    ")\n",
    "\n",
    "top_ten_pcsm['item'] = top_ten_pcsm.index.map(lambda x: code_description_map.loc[x])\n",
    "tt_pcsm = top_ten_pcsm.set_index('item', drop=True).sort_values(by=levels['city'], ascending=False)[:10]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "sns.heatmap(tt_pcsm.sort_values(by=levels['city'], ascending=False), cmap=cmap2, annot=True, annot_kws={'fontsize':12}, fmt='.3', ax=ax, square=True, cbar=False, linewidth=.05, linecolor='white')\n",
    "ax.tick_params(**ck.xlabels_top, **ck.no_xticks)\n",
    "plt.setp(ax.get_xticklabels(), rotation=90)\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_xlabel(\" \")\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ten objects that are found most frequently are not the same as the ten objects that are found in the greatest quantity. There are two objects, *foil wrappers* and *lolypop sticks*, that are found in approximately 75% of all surveys but not in quantities sufficient to make the top ten. \n",
    "\n",
    "Of those objects that had the greatest quantities, *nonfood packaging* and *styrofoam < 5mm*, were not identified as frequently as the other items but were found in quantities sufficient to place in the top ten.\n",
    "\n",
    "#### Definition: the most common objects\n",
    "\n",
    "__The most common objects are those objects that have a fail rate greater than 50% and/or are in the top-ten by quantity or pieces/m for any defined geographic area.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input",
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "fail_rate_ten = fail_rates_dfx.sort_values(by=\"Biel/Bienne\", ascending=False)[:10].index\n",
    "pcs_m_ten = tt_pcsm.index\n",
    "\n",
    "accounted = set([*top_ten_pcsm['Biel/Bienne'].sort_values(ascending=False)[:10].index, *the_top_ten])\n",
    "accounted_total = dfSurveys[(dfSurveys.city == 'Biel/Bienne')&(dfSurveys.code.isin(accounted))].quantity.sum()\n",
    "\n",
    "per_cent_all_biel = F\"\"\"\n",
    "\n",
    "*Combined, the most abundant objects and the objects found most frequently account for {np.ceil((accounted_total/biel_t)*100)}% of all objects found in Biel.*\n",
    "\"\"\"\n",
    "\n",
    "md(per_cent_all_biel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Percent of total: the compostion of the objects found\n",
    "\n",
    "The percent of total is the amount of an object found divided by the total amount of all objects found for a defined location/region and date range.\n",
    "\n",
    "**What does it mean?** The percent of total describes the quantity of objects found in relation to each other and the total amount of trash identified.\n",
    "\n",
    "> Use the percent of total to define the principal trash objects. Use percent of total to identify priorities on a regional scale\n",
    "\n",
    "The percent of total should always be evaluated with the pass-fail rate and the number of samples. Similar to pieces per meter, if an object has a low pass-fail rate and an elevated % of total it is a signal that objects are possibly being deposited in large quantities at irregular intervals: dumping or leaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "new_dfs = []\n",
    "for level in levels:\n",
    "    a_newdf = dfSurveys[dfSurveys[level] == levels[level]].groupby('code').agg({'quantity':'sum'})\n",
    "    a_newdf[levels[level]] = a_newdf/a_newdf.sum()\n",
    "    new_dfs.append(a_newdf[levels[level]])\n",
    "\n",
    "all_survey_areas = dfSurveys.groupby('code').agg({'quantity':'sum'})\n",
    "all_survey_areas['All survey areas'] = all_survey_areas/all_survey_areas.sum()\n",
    "new_dfs.append(all_survey_areas['All survey areas'])\n",
    "percent_of_total = pd.concat(new_dfs, axis=1)\n",
    "\n",
    "percent_of_total['item'] = percent_of_total.index.map(lambda x: code_description_map.loc[x])\n",
    "p_t = percent_of_total.loc[accounted].sort_values(by=levels['city'], ascending = False)\n",
    "p_t.set_index('item', inplace=True, drop=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,9))\n",
    "\n",
    "sns.heatmap(p_t, cmap=cmap2, annot=True, annot_kws={'fontsize':12},fmt='.0%', ax=ax, square=True, cbar=False, linewidth=.05, linecolor='white')\n",
    "ax.tick_params(**ck.xlabels_top, **ck.no_xticks)\n",
    "plt.setp(ax.get_xticklabels(), rotation=90)\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_xlabel(\" \")\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe the survey results using the key indicators\n",
    "\n",
    "Objects that are directly related to consumption (food, drinks, tobacco) are found at a rate that exceeds the median for the survey area, these objects represent 36% of the trash collected compared to 24% for all survey areas. Objects not directly related to consumption (insulation, expanded polystyrene, insultation) are found at a rate that exceeds the median for the survey area, these objects represent 22% of the total trash collected compared to 27% for all survey areas.\n",
    "\n",
    "Biel/Bienne is most likely a source of expanded polystyrene, cigarette ends and food wrappers given the elevated pcs/m and pass-fail rate when compared to the lake, the survey area or all survey areas. Expanded polystyrene is used as an exterior insulation envelope for buildings (new construction and renovations) and to protect component objects during transportation. Biel has a strong industrial base and an active construction industry, including many projects within 1.5km of the survey locations.\n",
    "\n",
    "In contrast, industrial sheeting and fragmented plastics have a pass-fail rate and median pcs/m value very similar to the rest of the lake, indicating that these values should be expected at other locations in the region. Thus, ruling out Biel/Bienne as a primary source of these two objects but definitely a zone of accumulation given the proximity of the Aare outflow and Suze inflow.\n",
    "\n",
    "__What about the rest?__\n",
    "\n",
    "The twelve objects indentified in the previous example account for approximately 66% (2019/3067) of all objects found in Biel/Bienne. That leaves 1048 objects that have not been accounted for. Of the 260 available object categories 114 were identified and 12 of those account for 66% of all the data. Therefore, there are 102 categories to describe the 1048 remaining objects, all of which are found less than 50% of the time and constitute a small portion of the total.\n",
    "\n",
    "The complete inventory of removed and indentified items is included with each data summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical excercise\n",
    "\n",
    "Plastic industrial pellets are the primary material used to produce plastic objects they are used extensively in Switzerland. They are disc or pellet shaped with a diameter of ~5mm.\n",
    "\n",
    "Given the following survey results, the map of survey locations and maintaining the assumptions presented at the begining of this article answer the following questions:\n",
    "\n",
    "1. Where are the best chances of finding at least one?\n",
    "2. If a survey of 50 meters is conducted what is the probable minimum amount of pellets would you find?\n",
    "3. Why did you pick that location or locations? How sure are you of your choices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "aggs = {'loc_date':'nunique', 'fail':'sum', 'pcs_m':'mean', \"quantity\":\"sum\"}\n",
    "new_col_names = {\"loc_date\":\"# of samples\", \"fail\":\"# fail\", \"pcs_m\":\"median pcs/m\", \"quantity\":\"#found\"}\n",
    "\n",
    "\n",
    "biel_g95 = dfSurveys[(dfSurveys.water_name_slug == levels['water_name_slug'])&(dfSurveys.code == 'G112')].groupby(['location']).agg(aggs)\n",
    "biel_g95.rename(columns=new_col_names, inplace=True)\n",
    "\n",
    "biel_g95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "bassin_map = PILImage.open(\"resources/maps/bielersee_scaled.jpeg\")\n",
    "\n",
    "# image = PImage.open('demo_image.jpg')\n",
    "bassin_map.thumbnail((800, 1200))\n",
    "# image.save('image_thumbnail.jpg')\n",
    "\n",
    "output = io.BytesIO()\n",
    "bassin_map.save(output, format='PNG')\n",
    "encoded_string = base64.b64encode(output.getvalue()).decode()\n",
    "\n",
    "html = '<img src=\"data:image/png;base64,{}\"/>'.format(encoded_string)\n",
    "IPython.display.HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "author = \"roger@hammerdirt.ch\"\n",
    "my_message = \"Love what you do. \\u2764\\ufe0f\"\n",
    "md(F\"\"\"\n",
    "**This project was made possible by the Swiss federal office for the environment.**<br>\n",
    "\n",
    ">{my_message}<br>\n",
    "\n",
    "*{author}* pushed the run button on {today}.<br>\n",
    "This document originates from https://github.com/hammerdirt-analyst/IQAASL-End-0f-Sampling-2021 all copyrights apply.<br>\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
